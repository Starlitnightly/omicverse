{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "973c5e83",
   "metadata": {},
   "source": [
    "# ov.Agent Ten-Task Regression Suite (Step-by-step)\n",
    "\n",
    "This notebook keeps things minimal: \n",
    "1. **Step 1** checks the runtime and initializes `ov.Agent`.\n",
    "2. **Step 2** fetches datasets directly via Scanpy or downloadable URLs.\n",
    "3. **Steps 3–13** each hold a standalone `ov.Agent(prompt, data)` block so you can trigger any task with a single cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e096e4a2",
   "metadata": {},
   "source": [
    "## Step 1 – Environment & agent check\n",
    "Import OmicVerse/Scanpy, confirm versions, and instantiate a single `ov.Agent` session that all later steps reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a650d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import scanpy as sc\n",
    "import omicverse as ov\n",
    "\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"OmicVerse version: {getattr(ov, '__version__', 'unknown')} @ {ov.__file__}\")\n",
    "print(f\"Scanpy version: {sc.__version__}\")\n",
    "print(\"\n",
    "Supported models:\")\n",
    "print(ov.list_supported_models())\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')\n",
    "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
    "api_key = OPENAI_API_KEY or ANTHROPIC_API_KEY or GEMINI_API_KEY\n",
    "if not api_key:\n",
    "    print('⚠️  Set OPENAI_API_KEY / ANTHROPIC_API_KEY / GEMINI_API_KEY before running the agent.')\n",
    "\n",
    "model_id = os.getenv('OV_AGENT_MODEL', 'gpt-5')\n",
    "sc.settings.set_figure_params(dpi=100)\n",
    "agent = ov.Agent(model=model_id, api_key=api_key)\n",
    "agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa14b653",
   "metadata": {},
   "source": [
    "## Step 2 – Download or stage datasets\n",
    "The cell below pulls what it can directly via `scanpy.datasets` and prints URL targets for heavier assets (Multiome, ATAC, TME, spatial, etc.). Set `AUTO_FETCH_WEB = True` if you want the script to download the external files automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fb912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import urllib.request\n",
    "\n",
    "DATA_ROOT = Path('ov_agent_data')\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SCANPY_DATASETS = {\n",
    "    'pbmc_fastq_qc': ('scanpy.datasets.pbmc3k', sc.datasets.pbmc3k),\n",
    "    'pancreas_multi_simba': ('scanpy.datasets.pbmc68k_reduced', sc.datasets.pbmc68k_reduced),\n",
    "    'paul15_traj': ('scanpy.datasets.paul15', sc.datasets.paul15),\n",
    "}\n",
    "\n",
    "DATA_CACHE = {}\n",
    "for key, (label, loader) in SCANPY_DATASETS.items():\n",
    "    try:\n",
    "        DATA_CACHE[key] = loader()\n",
    "        print(f\"✅ Loaded {key} via {label}\")\n",
    "    except Exception as exc:\n",
    "        DATA_CACHE[key] = None\n",
    "        print(f\"⚠️  Failed to load {key} via {label}: {exc}\")\n",
    "\n",
    "DATA_PATHS = {\n",
    "    'pbmc_multiome_rna': DATA_ROOT / 'pbmc_multiome_rna.h5ad',\n",
    "    'pbmc_multiome_atac': DATA_ROOT / 'pbmc_multiome_atac.h5ad',\n",
    "    'pbmc5k_atac_emb': DATA_ROOT / 'pbmc5k_atac_emb.h5ad',\n",
    "    'pbmc5k_rna_reference': DATA_ROOT / 'pbmc_rna_reference.h5ad',\n",
    "    'tme_cellphonedb': DATA_ROOT / 'tme_cellphonedb.h5ad',\n",
    "    'metatime_input': DATA_ROOT / 'metatime_input.h5ad',\n",
    "    'cefcon_input': DATA_ROOT / 'cefcon_nestorowa.h5ad',\n",
    "    'scdrug_input': DATA_ROOT / 'scdrug_scanpyobj.h5ad',\n",
    "    'visium_slice_151676': DATA_ROOT / '151676_filtered_feature_bc_matrix.h5',\n",
    "    'visium_slice_151507': DATA_ROOT / '151507_filtered_feature_bc_matrix.h5',\n",
    "}\n",
    "\n",
    "WEB_DATASETS = [\n",
    "    {\n",
    "        'name': 'pbmc_multiome_rna',\n",
    "        'url': 'https://figshare.com/ndownloader/files/41460054',\n",
    "        'path': DATA_PATHS['pbmc_multiome_rna'],\n",
    "    },\n",
    "    {\n",
    "        'name': 'pbmc_multiome_atac',\n",
    "        'url': 'https://figshare.com/ndownloader/files/41460051',\n",
    "        'path': DATA_PATHS['pbmc_multiome_atac'],\n",
    "    },\n",
    "    {\n",
    "        'name': 'pbmc5k_atac_emb',\n",
    "        'url': 'https://figshare.com/ndownloader/files/41418600',\n",
    "        'path': DATA_PATHS['pbmc5k_atac_emb'],\n",
    "    },\n",
    "    {\n",
    "        'name': 'pbmc5k_rna_reference',\n",
    "        'url': 'https://cf.10xgenomics.com/samples/cell-exp/1.1.0/pbmc3k/pbmc3k_filtered_gene_bc_matrices.tar.gz',\n",
    "        'path': DATA_PATHS['pbmc5k_rna_reference'],\n",
    "    },\n",
    "    {\n",
    "        'name': 'tme_cellphonedb',\n",
    "        'url': 'https://github.com/ventolab/CellphoneDB/raw/master/notebooks/data_tutorial.zip',\n",
    "        'path': DATA_PATHS['tme_cellphonedb'],\n",
    "    },\n",
    "    {\n",
    "        'name': 'metatime_input',\n",
    "        'url': 'https://figshare.com/ndownloader/files/41440050',\n",
    "        'path': DATA_PATHS['metatime_input'],\n",
    "    },\n",
    "    {\n",
    "        'name': 'cefcon_input',\n",
    "        'url': 'https://github.com/WPZgithub/CEFCON/raw/e74d2d248b88fb3349023d1a97d3cc8a52cc4060/notebooks/data/nestorowa16_preprocessed.h5ad',\n",
    "        'path': DATA_PATHS['cefcon_input'],\n",
    "    },\n",
    "    {\n",
    "        'name': 'scdrug_input',\n",
    "        'url': 'https://figshare.com/ndownloader/files/47461946',\n",
    "        'path': DATA_PATHS['scdrug_input'],\n",
    "    },\n",
    "    {\n",
    "        'name': 'visium_slice_151676',\n",
    "        'url': 'https://drive.google.com/uc?export=download&id=1Omte1adVFzyRDw7VloOAQYwtv_NjdWcG',\n",
    "        'path': DATA_PATHS['visium_slice_151676'],\n",
    "    },\n",
    "    {\n",
    "        'name': 'visium_slice_151507',\n",
    "        'url': 'https://drive.google.com/uc?export=download&id=1zsMZnG-tYr9ebquG6YULi6gvN00zGEGZ',\n",
    "        'path': DATA_PATHS['visium_slice_151507'],\n",
    "    },\n",
    "]\n",
    "\n",
    "def download_file(url: str, destination: Path) -> None:\n",
    "    destination.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if destination.exists():\n",
    "        print(f\"   ↪ {destination} already present\")\n",
    "        return\n",
    "    print(f\"   ↓ Downloading {url} → {destination}\")\n",
    "    with urllib.request.urlopen(url) as resp, open(destination, 'wb') as out:\n",
    "        out.write(resp.read())\n",
    "    print(f\"   ✅ Saved {destination}\")\n",
    "\n",
    "AUTO_FETCH_WEB = False\n",
    "if AUTO_FETCH_WEB:\n",
    "    for entry in WEB_DATASETS:\n",
    "        try:\n",
    "            download_file(entry['url'], entry['path'])\n",
    "        except Exception as exc:\n",
    "            print(f\"⚠️  Could not download {entry['name']}: {exc}\")\n",
    "else:\n",
    "    for entry in WEB_DATASETS:\n",
    "        print(f\"ℹ️  Stage {entry['name']} manually from {entry['url']} → {entry['path']}\")\n",
    "\n",
    "print(\"\n",
    "Dataset cache summary:\")\n",
    "for key, value in DATA_CACHE.items():\n",
    "    status = 'ready' if value is not None else 'missing'\n",
    "    print(f\" - {key}: {status}\")\n",
    "\n",
    "TASK_RESULTS = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee2677a",
   "metadata": {},
   "source": [
    "### Step 3 – PBMC 5k/8k FASTQ → QC → cluster-stability benchmarking\n",
    "Trigger the full PBMC end-to-end workflow with a single prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea6d79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "step3_prompt = \"\"\"You are ov.Agent orchestrating an end-to-end PBMC 5k/8k workflow.\n",
    "\n",
    "Use the skill registry to load and cite:\n",
    "- `single-preprocessing` for QC/HVG/scaling guidance.\n",
    "- `single-clustering` for multi-head resolution sweeps.\n",
    "- `data-viz-plots` plus `single-downstream-analysis` for stability diagnostics.\n",
    "\n",
    "Requirements:\n",
    "1. Start from raw 10x FASTQs, run kb-python alignment to generate `adata.h5ad` counts (show CLI commands).\n",
    "2. Apply PBMC-grade QC thresholds, normalization, and HVG selection, documenting parameter choices.\n",
    "3. Run Leiden, Louvain, Gaussian mixture, and LDA clustering across several resolutions, computing UMAP drift metrics that quantify stability.\n",
    "4. Return ordered code blocks plus a markdown table summarizing head, resolution, drift (0–1), and the recommended resolution.\"\"\"\n",
    "\n",
    "pbmc_fastq_data = DATA_CACHE.get('pbmc_fastq_qc')\n",
    "if pbmc_fastq_data is None:\n",
    "    raise RuntimeError('Load PBMC data via scanpy.datasets in Step 2 before running this cell.')\n",
    "TASK_RESULTS['pbmc_fastq_qc'] = agent.run(step3_prompt, pbmc_fastq_data)\n",
    "TASK_RESULTS['pbmc_fastq_qc']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0191227",
   "metadata": {},
   "source": [
    "### Step 4 – Pancreas multi-study merge with SIMBA embeddings\n",
    "Stress-test SIMBA-based integration on pancreas donors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac3955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "step4_prompt = \"\"\"Integrate the Baron, Segerstolpe, and Muraro pancreas scRNA-seq donors under strong batch effects.\n",
    "\n",
    "Use the registry to load:\n",
    "- `single-preprocessing` for donor-level normalization and covariate regression.\n",
    "- `single-multiomics` for SIMBA-style heterogeneous graph construction.\n",
    "- `single-clustering` and `data-viz-plots` for UMAP diagnostics.\n",
    "\n",
    "Workflow expectations:\n",
    "1. Highlight how to harmonize preprocessing parameters across cohorts before building the SIMBA graph.\n",
    "2. Provide the concrete SIMBA commands that add nodes/edges, train embeddings, and export 2D projections.\n",
    "3. Quantify integration with before/after UMAPs, kBET, and silhouette scores, commenting on endocrine vs. exocrine separation.\n",
    "4. Explain how to persist the learned embeddings for downstream classifiers.\"\"\"\n",
    "\n",
    "pancreas_data = DATA_CACHE.get('pancreas_multi_simba')\n",
    "if pancreas_data is None:\n",
    "    raise RuntimeError('Stage a pancreas multi-donor AnnData object in DATA_CACHE['pancreas_multi_simba'].')\n",
    "TASK_RESULTS['pancreas_multi_simba'] = agent.run(step4_prompt, pancreas_data)\n",
    "TASK_RESULTS['pancreas_multi_simba']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d7449c",
   "metadata": {},
   "source": [
    "### Step 5 – Paul15 hematopoietic trajectories with MetaTiME diagnostics\n",
    "Rebuild hematopoietic trajectories plus MetaTiME cycle checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a020640",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "step5_prompt = \"\"\"Reconstruct megakaryocyte vs. lymphoid trajectories on Paul15-like hematopoiesis data.\n",
    "\n",
    "Use the registry to pull:\n",
    "- `single-trajectory` for diffusion, PAGA, Palantir/VIA, and MetaTiME checkpoints.\n",
    "- `single-preprocessing` for QC and normalization.\n",
    "- `single-downstream-analysis` plus `data-viz-plots` for marker summaries.\n",
    "\n",
    "Deliverables:\n",
    "1. Describe preprocessing plus neighborhood graph construction before diffusion/PAGA.\n",
    "2. Run diffusion maps, Palantir/VIA, and identify root and terminal states with pseudotime ordering.\n",
    "3. Produce branch marker tables for at least two fates and explain MetaTiME cycle diagnostics that validate the ordering.\n",
    "4. Return code snippets, saved-figure descriptions, and a markdown list of pseudotime milestones.\"\"\"\n",
    "\n",
    "paul15_data = DATA_CACHE.get('paul15_traj')\n",
    "if paul15_data is None:\n",
    "    raise RuntimeError('Paul15 data is missing—rerun Step 2 or provide your own AnnData object.')\n",
    "TASK_RESULTS['paul15_traj'] = agent.run(step5_prompt, paul15_data)\n",
    "TASK_RESULTS['paul15_traj']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2624f0c",
   "metadata": {},
   "source": [
    "### Step 6 – PBMC Multiome 10k GLUE + MOFA factor discovery\n",
    "Align RNA/ATAC embeddings and decode factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7bd746",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "step6_prompt = \"\"\"Perform cross-modal alignment for PBMC Multiome 10k.\n",
    "\n",
    "Use skill registry lookups for:\n",
    "- `single-multiomics` (GLUE pairing + MOFA training).\n",
    "- `single-preprocessing` (modality-specific normalization).\n",
    "- `data-viz-plots` (factor visualization).\n",
    "\n",
    "Tasks:\n",
    "1. Pair RNA and ATAC embeddings with GLUE and report the path to the paired metadata.\n",
    "2. Train MOFA on matched matrices, labelling shared, RNA-only, and ATAC-only factors with variance explained tables.\n",
    "3. Highlight at least one IFN-response and one chromatin-accessibility-specific factor, with marker genes/peaks.\n",
    "4. Provide code snippets plus interpretation bullets for each factor category.\"\"\"\n",
    "\n",
    "multiome_rna = DATA_PATHS['pbmc_multiome_rna']\n",
    "multiome_atac = DATA_PATHS['pbmc_multiome_atac']\n",
    "if not multiome_rna.exists() or not multiome_atac.exists():\n",
    "    raise FileNotFoundError('Download both RNA and ATAC embeddings in Step 2 before running this cell.')\n",
    "multiome_data = sc.read(multiome_rna)\n",
    "multiome_data.uns['atac_embedding_path'] = str(multiome_atac)\n",
    "TASK_RESULTS['multiome_glue'] = agent.run(step6_prompt, multiome_data)\n",
    "TASK_RESULTS['multiome_glue']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3e1b09",
   "metadata": {},
   "source": [
    "### Step 7 – PBMC 5k scATAC label transfer via GLUE embeddings\n",
    "Move annotations from RNA to ATAC with cross-modal KNN graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff48d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "step7_prompt = \"\"\"Transfer PBMC RNA annotations onto PBMC 5k scATAC cells.\n",
    "\n",
    "Use skill registry entries:\n",
    "- `single-multiomics` for GLUE-derived embeddings and cross-modal graphs.\n",
    "- `single-annotation` for label transfer/validation patterns.\n",
    "\n",
    "Instructions:\n",
    "1. Load the GLUE embeddings (`data/analysis_lymph/rna-emb.h5ad` and `data/analysis_lymph/atac-emb.h5ad`).\n",
    "2. Build the cross-modal KNN graph, migrate labels with confidence, and surface per-cluster agreement statistics.\n",
    "3. Flag potential mismatches and explain how to visualize transferred labels on ATAC UMAPs.\n",
    "4. Return python commands plus a markdown table of cluster vs. confidence.\"\"\"\n",
    "\n",
    "atac_path = DATA_PATHS['pbmc5k_atac_emb']\n",
    "rna_ref_path = DATA_PATHS['pbmc5k_rna_reference']\n",
    "if not atac_path.exists() or not rna_ref_path.exists():\n",
    "    raise FileNotFoundError('Stage both ATAC and RNA reference embeddings before running the transfer cell.')\n",
    "atac_data = sc.read(atac_path)\n",
    "atac_data.uns['rna_reference_path'] = str(rna_ref_path)\n",
    "TASK_RESULTS['pbmc5k_scatac_transfer'] = agent.run(step7_prompt, atac_data)\n",
    "TASK_RESULTS['pbmc5k_scatac_transfer']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3842255c",
   "metadata": {},
   "source": [
    "### Step 8 – Tumor microenvironment ligand–receptor diagnostics (CellPhoneDBViz)\n",
    "Compare exhausted T cells vs. M2 macrophages across treatments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92d7a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "step8_prompt = \"\"\"Contrast ligand–receptor usage between exhausted T cells and M2 macrophages for treated vs. untreated tumors.\n",
    "\n",
    "Use registry calls for:\n",
    "- `single-cellphone-db` (interaction formatting, execution, visualization).\n",
    "- `single-downstream-analysis` and `data-viz-plots` (interpretation + figure export).\n",
    "\n",
    "Expectations:\n",
    "1. Show metadata formatting, CellPhoneDB execution commands, and result parsing.\n",
    "2. Highlight top ligand–receptor pairs per condition with effect sizes/p-values.\n",
    "3. Describe how to build heatmaps and chord diagrams (include filenames) via CellPhoneDBViz helpers.\n",
    "4. Provide interpretation guidance on shifts between conditions.\"\"\"\n",
    "\n",
    "tme_path = DATA_PATHS['tme_cellphonedb']\n",
    "if not tme_path.exists():\n",
    "    raise FileNotFoundError('Download the CellPhoneDB-ready AnnData file before running this cell.')\n",
    "tme_data = sc.read(tme_path)\n",
    "TASK_RESULTS['cellphonedb'] = agent.run(step8_prompt, tme_data)\n",
    "TASK_RESULTS['cellphonedb']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7012cd",
   "metadata": {},
   "source": [
    "### Step 9 – MetaTiME-driven immune microenvironment annotation\n",
    "Score immune states with MetaTiME meta-components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4da838",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "step9_prompt = \"\"\"Annotate tumor-infiltrating immune cells with MetaTiME.\n",
    "\n",
    "Use skill registry lookups for:\n",
    "- `single-trajectory` (MetaTiME scoring + pseudotime context).\n",
    "- `single-preprocessing` (optional inferCNV-based malignant removal).\n",
    "- `single-downstream-analysis` (report formatting).\n",
    "\n",
    "Deliverables:\n",
    "1. Optionally filter malignant cells via infercnvpy outputs and recompute neighbors in SCVI space.\n",
    "2. Run MetaTiME scoring, rank meta-components per cluster, and interpret dominant immune states.\n",
    "3. Provide preprocessing + scoring code plus a markdown report mapping cluster → top meta-component with interpretation.\"\"\"\n",
    "\n",
    "metatime_path = DATA_PATHS['metatime_input']\n",
    "if not metatime_path.exists():\n",
    "    raise FileNotFoundError('Provide the MetaTiME-ready AnnData file (TiME_adata_scvi.h5ad) before running this cell.')\n",
    "metatime_data = sc.read(metatime_path)\n",
    "TASK_RESULTS['metatime'] = agent.run(step9_prompt, metatime_data)\n",
    "TASK_RESULTS['metatime']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fc7993",
   "metadata": {},
   "source": [
    "### Step 10 – CEFCON driver regulator discovery\n",
    "Find branch-specific regulators with the CEFCON pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d4f71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "step10_prompt = \"\"\"Discover lineage-specific driver regulators with CEFCON on Nestorowa/Paul15 hematopoiesis.\n",
    "\n",
    "Use the registry to load:\n",
    "- `single-trajectory` (fate modeling context).\n",
    "- `single-downstream-analysis` (regulator reporting).\n",
    "\n",
    "Steps:\n",
    "1. Document preprocessing and prior network setup, including how to load the NicheNet graph.\n",
    "2. Run `ov.single.pyCEFCON`, exporting regulon tables for at least two branches (erythroid vs. granulocyte).\n",
    "3. Provide tuning advice (walk length, regularization) and show how to visualize regulator activity heatmaps.\n",
    "4. Summarize key regulators per branch in markdown.\"\"\"\n",
    "\n",
    "cefcon_path = DATA_PATHS['cefcon_input']\n",
    "if not cefcon_path.exists():\n",
    "    raise FileNotFoundError('Download the Nestorowa/Paul15 preprocessed AnnData for CEFCON before running this cell.')\n",
    "cefcon_data = sc.read(cefcon_path)\n",
    "TASK_RESULTS['cefcon'] = agent.run(step10_prompt, cefcon_data)\n",
    "TASK_RESULTS['cefcon']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a815a7f",
   "metadata": {},
   "source": [
    "### Step 11 – Precision oncology prioritization (inferCNV + scDrug)\n",
    "Rank therapies per malignant clone after inferCNV calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16011a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "step11_prompt = \"\"\"Combine inferCNV-based malignant calling with scDrug predictions for precision oncology.\n",
    "\n",
    "Use registry lookups for:\n",
    "- `single-multiomics` (scDrug + modality handling).\n",
    "- `single-downstream-analysis` (drug ranking summaries).\n",
    "- `data-viz-plots` (CNV heatmap references).\n",
    "\n",
    "Workflow:\n",
    "1. Run infercnvpy to separate malignant vs. normal cells and reference the CNV heatmap artifact.\n",
    "2. Feed malignant clones into scDrug, compute predicted IC50 values, and rank at least five compounds per clone.\n",
    "3. Provide code for exporting the ranking table plus guidance on cross-referencing copy-number context when interpreting drug hits.\"\"\"\n",
    "\n",
    "scdrug_path = DATA_PATHS['scdrug_input']\n",
    "if not scdrug_path.exists():\n",
    "    raise FileNotFoundError('Stage the scanpyobj.h5ad file before invoking the scDrug workflow.')\n",
    "scdrug_data = sc.read(scdrug_path)\n",
    "TASK_RESULTS['scdrug'] = agent.run(step11_prompt, scdrug_data)\n",
    "TASK_RESULTS['scdrug']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f43d2cd",
   "metadata": {},
   "source": [
    "### Step 12 – SpaceFlow pseudo-spatiotemporal mapping (Visium 151676)\n",
    "Derive domains and pSM layers from a single Visium slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f064bafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "step12_prompt = \"\"\"Compute SpaceFlow embeddings and pseudo-spatiotemporal maps for Visium DLPFC slice 151676.\n",
    "\n",
    "Use registry entries for:\n",
    "- `single-to-spatial-mapping` (Visium pre-processing).\n",
    "- `spatial-trajectory` (SpaceFlow training + domain discovery).\n",
    "- `data-viz-plots` (domain overlays on histology).\n",
    "\n",
    "Plan:\n",
    "1. Load `151676_filtered_feature_bc_matrix.h5`, normalize spots, and build spatial KNN graphs for SpaceFlow.\n",
    "2. Train SpaceFlow, report domain assignments plus pseudo-spatiotemporal maps (pSM) and save the embeddings/pSM matrices.\n",
    "3. Describe how to visualize embeddings/domains layered on histology with filenames for the exported figures.\"\"\"\n",
    "\n",
    "spaceflow_path = DATA_PATHS['visium_slice_151676']\n",
    "if not spaceflow_path.exists():\n",
    "    raise FileNotFoundError('Download the 151676 Visium matrix (.h5) before running SpaceFlow.')\n",
    "spaceflow_data = sc.read_10x_h5(spaceflow_path)\n",
    "TASK_RESULTS['spaceflow_151676'] = agent.run(step12_prompt, spaceflow_data)\n",
    "TASK_RESULTS['spaceflow_151676']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94efcd92",
   "metadata": {},
   "source": [
    "### Step 13 – STAligner multi-slice alignment (151676 ↔ 151507)\n",
    "Align consecutive Visium sections with triplet-loss GATs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9726767",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "step13_prompt = \"\"\"Align consecutive Visium DLPFC slices (151676 and 151507) with STAligner.\n",
    "\n",
    "Use the skill registry to load:\n",
    "- `spatial-alignment` (STAligner configuration and triplet-loss GAT training).\n",
    "- `single-to-spatial-mapping` (Visium preprocessing for both slices).\n",
    "- `data-viz-plots` (aligned cortical-layer visualization).\n",
    "\n",
    "Expectations:\n",
    "1. Preprocess both filtered matrices, harmonize features, and construct spot graphs before alignment.\n",
    "2. Run STAligner with triplet-loss GAT, report where embeddings and aligned coordinates are saved, and describe convergence checks.\n",
    "3. Summarize conserved cortical layers across slices plus any mismatches, pointing to exported alignment plots.\"\"\"\n",
    "\n",
    "slice_a = DATA_PATHS['visium_slice_151676']\n",
    "slice_b = DATA_PATHS['visium_slice_151507']\n",
    "if not slice_a.exists() or not slice_b.exists():\n",
    "    raise FileNotFoundError('Download both Visium slices (151676 & 151507) before running STAligner.')\n",
    "slice_a_data = sc.read_10x_h5(slice_a)\n",
    "slice_a_data.uns['staligner_peer_slice_path'] = str(slice_b)\n",
    "TASK_RESULTS['staligner'] = agent.run(step13_prompt, slice_a_data)\n",
    "TASK_RESULTS['staligner']\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
