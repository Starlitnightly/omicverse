# Bug Report: Prompt Length Limit Exceeded in OVAgent

**Discovered**: 2025-11-12 01:12:02
**Severity**: CRITICAL - Blocks all agent functionality
**Status**: Confirmed across all providers (OpenAI, Gemini)

---

## Summary

The OmicVerse Agent fails on **ALL requests** (even simple ones) due to the generated prompt exceeding the hardcoded 100,000 character limit.

```
âŒ ERROR: user_prompt too long (124017 chars, max 100000)
```

---

## Reproduction

### Environment
- OmicVerse: 1.7.9rc1
- Python: 3.12.12
- Test: `test_ovagent_openai_gemini.py`

### Steps to Reproduce
1. Initialize agent: `agent = ov.Agent(model='gpt-5', api_key=api_key)`
2. Make ANY request: `agent.run('quality control with nUMI>500, mito<0.2', adata)`
3. Error occurs in both Priority 1 and Priority 2 workflows

### Expected Behavior
- Agent should process the request
- Prompt should fit within limits

### Actual Behavior
```
âš ï¸  Priority 1 insufficient: user_prompt too long (124304 chars, max 100000)
ðŸ”„ Falling back to Priority 2 (skills-guided workflow)...
âŒ ERROR - Priority 2 failed
Error: user_prompt too long (124017 chars, max 100000)
```

---

## Root Cause Analysis

### Location
**File**: `omicverse/utils/agent_backend.py`
**Lines**: 331-333, 373-375

```python
if len(user_prompt) > 100000:
    raise ValueError(
        f"user_prompt too long ({len(user_prompt)} chars, max 100000)"
    )
```

### Why It Happens

1. **Function Registry Size**:
   - Registry contains 110 functions across 7 categories
   - When serialized into prompt: **~124,000 characters**
   - Hardcoded limit: **100,000 characters**

2. **Both Workflows Affected**:
   - **Priority 1** (fast registry-based): 124,304 chars
   - **Priority 2** (skills-guided): 124,017 chars

3. **Happens for ALL Requests**:
   - Even simple requests like "compute pca"
   - Both simple and complex tasks fail
   - All providers affected (OpenAI, Gemini, Anthropic, etc.)

---

## Impact

### Affected Functionality
- âœ… Agent initialization: **Works**
- âŒ Quality control: **Fails**
- âŒ Preprocessing: **Fails**
- âŒ Clustering: **Fails**
- âŒ Visualization: **Fails**
- âŒ All agent.run() calls: **Fail**

### Affected Providers
- âŒ OpenAI (GPT-5, GPT-4o-mini)
- âŒ Gemini (2.5 Pro, 2.5 Flash)
- âŒ Anthropic (Claude models)
- âŒ All other providers

### Test Results
```
Overall Test Results:
  Total Tests Run:    8
  Tests Passed:       4 âœ…  (only initialization)
  Tests Failed:       4 âŒ  (all processing requests)
  Pass Rate:          50.0%
```

---

## Solutions

### Option 1: Increase Limit (Quick Fix) âš¡

**Change**:
```python
# agent_backend.py line 331
if len(user_prompt) > 200000:  # Increased from 100000
    raise ValueError(
        f"user_prompt too long ({len(user_prompt)} chars, max 200000)"
    )
```

**Pros**:
- Immediate fix
- Minimal code changes

**Cons**:
- Doesn't address root cause
- Higher token costs
- May hit LLM context limits

---

### Option 2: Reduce Function Registry in Prompts (Recommended) âœ…

**Strategy**: Use progressive disclosure for functions (like skills)

**Implementation**:
1. Load only function metadata at startup (name + brief description)
2. Lazy-load full function details when needed
3. Only include relevant functions in each request

**Changes needed**:
- Modify function registry serialization in `smart_agent.py`
- Implement selective function loading based on request
- Add function matching logic (similar to skill matching)

**Benefits**:
- Reduces prompt from ~124K to ~20K chars
- Faster execution
- Lower token costs
- Scales to more functions

**Example**:
```python
# Current: Include ALL 110 functions in every prompt
function_registry_text = serialize_all_functions()  # 100K+ chars

# Proposed: Include only relevant functions
matched_functions = match_functions_to_request(request)  # 5-10K chars
function_text = serialize_functions(matched_functions)
```

---

### Option 3: Compress Function Descriptions ðŸ“

**Strategy**: Reduce verbosity of function descriptions

**Implementation**:
- Shorten function docstrings in prompts
- Use abbreviations and compact formatting
- Remove examples from prompt (keep in code docs)

**Expected Savings**: ~30-40% reduction (still may not be enough)

---

### Option 4: Two-Phase Execution ðŸ”„

**Strategy**: First choose functions, then execute

**Flow**:
1. **Phase 1**: Match request to functions (small prompt)
2. **Phase 2**: Load matched functions and execute (focused prompt)

**Benefits**:
- Clean separation of concerns
- Better traceability
- Aligns with LLM-based skill matching approach

---

## Recommended Fix (Immediate)

### Step 1: Quick Patch (5 minutes)

**File**: `omicverse/utils/agent_backend.py`

```python
# Line 331 (and 373) - change limit from 100000 to 200000
if len(user_prompt) > 200000:
    raise ValueError(
        f"user_prompt too long ({len(user_prompt)} chars, max 200000)"
    )
```

This allows testing to proceed immediately.

### Step 2: Proper Fix (1-2 hours)

Implement progressive function loading in `smart_agent.py`:

1. **Create Function Metadata Class**:
```python
@dataclass
class FunctionMetadata:
    name: str
    category: str
    brief_description: str  # Max 100 chars
    path: str
```

2. **Modify FunctionRegistry**:
```python
class FunctionRegistry:
    def __init__(self):
        self.metadata = {}  # Light load at startup
        self.full_functions = {}  # Lazy load on demand

    def get_metadata_summary(self) -> str:
        """Returns ~5K char summary instead of 100K"""
        return "\n".join([
            f"{m.name}: {m.brief_description}"
            for m in self.metadata.values()
        ])

    def load_function(self, name: str):
        """Lazy load full function details"""
        if name not in self.full_functions:
            self.full_functions[name] = self._load_from_disk(name)
        return self.full_functions[name]
```

3. **Update Prompt Building**:
```python
# Instead of including all 110 functions:
matched_functions = await self._match_functions_to_request(request)
function_prompt = self._build_function_prompt(matched_functions)
# Now ~10K chars instead of 124K
```

---

## Testing the Fix

### After Quick Patch

```bash
# Apply the 200,000 char limit increase
# Then run tests:
python3 test_ovagent_openai_gemini.py
```

**Expected**:
- All initialization tests: âœ… Pass
- Quality control: âœ… Pass
- Preprocessing: âœ… Pass
- Clustering: âœ… Pass
- UMAP: âœ… Pass
- End-to-end: âœ… Pass

### After Proper Fix

```bash
# Test that prompt is now smaller
python3 -c "
import omicverse as ov
agent = ov.Agent(model='gpt-4o-mini', api_key='test')
# Check that function prompt is < 50K chars
"
```

---

## Additional Observations

### 1. Missing Skills Directory
```
âš ï¸  Built-in skills directory not found:
/Users/.../omicverse/omicverse/.claude/skills
```

**Impact**: Skills not loaded, but not causing the main failure

**Fix**: Ensure `.claude/skills/` directory exists in package

### 2. Progressive Disclosure Already Implemented for Skills

The codebase **already has** progressive disclosure for skills:
- Skills load metadata only at startup
- Full content lazy-loaded when matched
- This is the model to follow for functions!

**Reference**: `omicverse/utils/skill_registry.py`

---

## Priority

**CRITICAL** - This bug blocks all agent functionality.

### Immediate Action Required
1. Apply quick patch (increase limit to 200,000)
2. Test with `test_ovagent_openai_gemini.py`
3. Plan proper fix for next release

### Next Release
1. Implement progressive function loading
2. Reduce prompt sizes across the board
3. Add telemetry to monitor prompt sizes

---

## Workarounds (for Users)

**Currently**: None. Users cannot work around this bug.

**After quick patch applied**: Users can upgrade to patched version.

---

## Related Files

- `omicverse/utils/agent_backend.py` - Contains hardcoded limit
- `omicverse/utils/smart_agent.py` - Builds prompts with full function registry
- `omicverse/utils/function_registry.py` - (if exists) Function registry implementation
- `omicverse/utils/skill_registry.py` - Example of progressive disclosure (good model)

---

## Test Evidence

**Full test output**: See `test_results_20251112_011202.txt`

**Key excerpts**:
```
ðŸš€ Priority 1: Fast registry-based workflow
   ðŸ’­ Generating code with registry functions only...

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âš ï¸  Priority 1 insufficient: user_prompt too long (124304 chars, max 100000)
ðŸ”„ Falling back to Priority 2 (skills-guided workflow)...
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ðŸ’¡ Strategy: Priority 2 (fallback from Priority 1)

ðŸ§  Priority 2: Skills-guided workflow for complex tasks
   ðŸŽ¯ Matching relevant skills...
   ðŸ’­ Generating multi-step workflow code...

======================================================================
âŒ ERROR - Priority 2 failed
Error: user_prompt too long (124017 chars, max 100000)
======================================================================
```

**Reproduced on**:
- âœ… OpenAI GPT-5
- âœ… OpenAI GPT-4o-mini
- âœ… Gemini 2.5 Pro
- âœ… Gemini 2.5 Flash

**100% reproduction rate** across all tested models.

---

## Conclusion

This is a **critical architectural issue** that prevents the agent from functioning. The quick fix (increasing the limit) will unblock users immediately, but the proper fix (progressive function loading) should be implemented to ensure long-term scalability and performance.

The good news: The codebase already demonstrates the right pattern with progressive skill loading. Applying the same approach to functions will resolve this issue permanently.

---

**Report prepared by**: Automated testing suite
**Date**: 2025-11-12
**Test script**: `test_ovagent_openai_gemini.py`
