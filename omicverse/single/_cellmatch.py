from __future__ import annotations
import json
import logging
import os
import pickle
import re
from pathlib import Path
import networkx as nx
import numpy as np
import pandas as pd
from anndata import AnnData
from scipy.sparse import csr_matrix
from sklearn.metrics.pairwise import cosine_similarity

from tqdm import tqdm

class CellOntologyMapper:
    """
    ğŸ§¬ Cell ontology mapping class using NLP
    """
    
    def __init__(self, cl_obo_file=None, embeddings_path=None, model_name="all-mpnet-base-v2", local_model_dir=None):
        """
        ğŸš€ Initialize CellOntologyMapper
        
        Parameters
        ----------
        cl_obo_file : str, optional
            ğŸ“„ Cell Ontology OBO file path
        embeddings_path : str, optional
            ğŸ’¾ Pre-computed embeddings file path
        model_name : str
            ğŸ¤– Sentence Transformer model name
        local_model_dir : str, optional
            ğŸ“ Local directory to save downloaded models (avoid default cache)
        """
        self.model_name = model_name
        self.model = None
        self.local_model_dir = local_model_dir
        self.ontology_embeddings = None
        self.ontology_labels = None
        self.popv_dict = None
        self.output_path = None
        
        # LLM expansion functionality related
        self.llm_client = None
        self.llm_config = None
        self.abbreviation_cache = {}
        self.cache_file = None
        
        # Context information
        self.tissue_context = None
        self.species = "human"
        self.study_context = None
        
        # Initialize based on provided parameters
        if embeddings_path and os.path.exists(embeddings_path):
            print("ğŸ“¥ Loading existing ontology embeddings...")
            self.load_embeddings(embeddings_path)
        elif cl_obo_file and os.path.exists(cl_obo_file):
            print("ğŸ”¨ Creating ontology resources from OBO file...")
            self.create_ontology_resources(cl_obo_file)
        else:
            print("?  Initialized empty mapper, please use load_embeddings() or create_ontology_resources()")
    
    def setup_llm_expansion(self, api_type="openai", api_key=None, model="gpt-3.5-turbo", 
                           base_url=None, cache_file="abbreviation_cache.json", 
                           tissue_context=None, species="human", study_context=None,
                           extra_params=None):
        """
        ğŸ¤– Setup LLM API for abbreviation expansion
        
        Parameters
        ----------
        api_type : str
            ğŸ”Œ API type: "openai", "anthropic", "ollama", "qwen", "ernie", "glm", "spark", "doubao", "custom_openai"
        api_key : str, optional
            ğŸ” API key
        model : str
            ğŸ§  Model name
        base_url : str, optional
            ğŸŒ Custom API base URL (required for custom_openai and some domestic models)
        cache_file : str
            ğŸ’½ Cache file path
        tissue_context : str or list, optional
            ğŸ§¬ Tissue context information, e.g. "immune system", "brain", "liver" or list of tissues
        species : str
            ğŸ­ Species, default "human", can also be "mouse", "rat", etc.
        study_context : str, optional
            ğŸ”¬ Study context, e.g. "cancer", "development", "aging", "disease", etc.
        extra_params : dict, optional
            ğŸ”§ Additional parameters for specific APIs
        """
        self.llm_config = {
            'api_type': api_type,
            'api_key': api_key,
            'model': model,
            'base_url': base_url,
            'extra_params': extra_params or {}
        }
        
        # New context information
        self.tissue_context = tissue_context
        self.species = species
        self.study_context = study_context
        
        self.cache_file = cache_file
        self._load_abbreviation_cache()
        
        # Initialize client
        try:
            if api_type == "openai":
                import openai
                if base_url:
                    self.llm_client = openai.OpenAI(api_key=api_key, base_url=base_url)
                else:
                    self.llm_client = openai.OpenAI(api_key=api_key)
                    
            elif api_type == "custom_openai":
                # Generic OpenAI-compatible API with custom base_url
                import openai
                if not base_url:
                    raise ValueError("ğŸŒ base_url is required for custom_openai API type")
                self.llm_client = openai.OpenAI(api_key=api_key, base_url=base_url)
                
            elif api_type == "anthropic":
                import anthropic
                self.llm_client = anthropic.Anthropic(api_key=api_key)
                
            elif api_type == "ollama":
                import requests
                self.llm_client = "ollama"  # Mark as ollama, use requests
                self.base_url = base_url or "http://localhost:11434"
                
            elif api_type == "qwen":
                # é˜¿é‡Œäº‘é€šä¹‰åƒé—® (DashScope)
                try:
                    import dashscope
                    if api_key:
                        dashscope.api_key = api_key
                    elif 'DASHSCOPE_API_KEY' not in os.environ:
                        raise ValueError("ğŸ”‘ API key required for Qwen. Set api_key parameter or DASHSCOPE_API_KEY environment variable")
                    self.llm_client = "qwen"
                    print("ğŸ¤– Using DashScope API (é€šä¹‰åƒé—®)")
                except ImportError:
                    print("ğŸ“¦ Installing DashScope SDK: pip install dashscope")
                    raise ImportError("Please install: pip install dashscope")
                    
            elif api_type == "ernie":
                # ç™¾åº¦æ–‡å¿ƒä¸€è¨€
                try:
                    import ernie
                    if api_key:
                        # Assume api_key is in format "access_key:secret_key"
                        if ':' in api_key:
                            access_key, secret_key = api_key.split(':', 1)
                            ernie.api_type = 'aistudio'
                            ernie.access_token = access_key
                        else:
                            ernie.api_type = 'aistudio'  
                            ernie.access_token = api_key
                    self.llm_client = "ernie"
                    print("ğŸ¤– Using ERNIE API (æ–‡å¿ƒä¸€è¨€)")
                except ImportError:
                    print("ğŸ“¦ Installing ERNIE SDK: pip install ernie-bot-sdk")
                    raise ImportError("Please install: pip install ernie-bot-sdk")
                    
            elif api_type == "glm":
                # æ™ºè°±AI GLM
                try:
                    import zhipuai
                    if api_key:
                        zhipuai.api_key = api_key
                    elif 'ZHIPUAI_API_KEY' not in os.environ:
                        raise ValueError("ğŸ”‘ API key required for GLM. Set api_key parameter or ZHIPUAI_API_KEY environment variable")
                    self.llm_client = "glm"
                    print("ğŸ¤– Using ZhipuAI API (æ™ºè°±GLM)")
                except ImportError:
                    print("ğŸ“¦ Installing ZhipuAI SDK: pip install zhipuai")
                    raise ImportError("Please install: pip install zhipuai")
                    
            elif api_type == "spark":
                # è®¯é£æ˜Ÿç«
                import requests
                if not api_key:
                    raise ValueError("ğŸ”‘ API key required for Spark API")
                # Assume api_key format: "app_id:api_key:api_secret"
                if api_key.count(':') != 2:
                    raise ValueError("ğŸ”‘ Spark API key should be in format: 'app_id:api_key:api_secret'")
                app_id, spark_api_key, api_secret = api_key.split(':')
                self.llm_config['app_id'] = app_id
                self.llm_config['spark_api_key'] = spark_api_key
                self.llm_config['api_secret'] = api_secret
                self.llm_client = "spark"
                print("ğŸ¤– Using iFlytek Spark API (è®¯é£æ˜Ÿç«)")
                
            elif api_type == "doubao":
                # å­—èŠ‚è·³åŠ¨è±†åŒ… (ä½¿ç”¨é€šç”¨OpenAIå…¼å®¹æ ¼å¼)
                import openai
                default_base_url = "https://ark.cn-beijing.volces.com/api/v3"
                actual_base_url = base_url or default_base_url
                self.llm_client = openai.OpenAI(api_key=api_key, base_url=actual_base_url)
                print(f"ğŸ¤– Using Doubao API (è±†åŒ…) - Base URL: {actual_base_url}")
                
            else:
                supported_types = ["openai", "custom_openai", "anthropic", "ollama", "qwen", "ernie", "glm", "spark", "doubao"]
                print(f"âœ— Unsupported API type: {api_type}")
                print(f"ğŸ’¡ Supported types: {', '.join(supported_types)}")
                return False
            
            print(f"âœ“ LLM expansion functionality setup complete (Type: {api_type}, Model: {model})")
            if base_url and api_type not in ["doubao"]:  # doubao already prints base_url
                print(f"ğŸŒ Custom Base URL: {base_url}")
            if tissue_context:
                print(f"ğŸ§¬ Tissue context: {tissue_context}")
            if study_context:
                print(f"ğŸ”¬ Study context: {study_context}")
            print(f"ğŸ­ Species: {species}")
            return True
            
        except ImportError as e:
            print(f"âœ— Missing required library: {e}")
            print("ğŸ“¦ Install required packages based on your API type:")
            print("   - OpenAI: pip install openai")
            print("   - Anthropic: pip install anthropic")
            print("   - é€šä¹‰åƒé—®: pip install dashscope")
            print("   - æ–‡å¿ƒä¸€è¨€: pip install ernie-bot-sdk")
            print("   - æ™ºè°±GLM: pip install zhipuai")
            print("   - Ollama/Spark/Doubao: pip install requests")
            return False
        except Exception as e:
            print(f"âœ— LLM setup failed: {e}")
            return False
    
    def _load_abbreviation_cache(self):
        """ğŸ“¥ Load abbreviation cache"""
        if self.cache_file and os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    self.abbreviation_cache = json.load(f)
                print(f"âœ“ Loaded {len(self.abbreviation_cache)} cached abbreviation expansions")
            except:
                self.abbreviation_cache = {}
        else:
            self.abbreviation_cache = {}
    
    def _save_abbreviation_cache(self):
        """ğŸ’¾ Save abbreviation cache"""
        if self.cache_file:
            try:
                with open(self.cache_file, 'w', encoding='utf-8') as f:
                    json.dump(self.abbreviation_cache, f, indent=2, ensure_ascii=False)
            except Exception as e:
                print(f"âœ— Failed to save cache: {e}")
    
    def _is_likely_abbreviation(self, cell_name):
        """ğŸ” Determine if it's likely an abbreviation (improved pattern matching)"""
        cell_name = cell_name.strip()
        
        # å¦‚æœä¸ºç©ºæˆ–è¿‡é•¿ï¼Œä¸å¤ªå¯èƒ½æ˜¯ç¼©å†™
        if not cell_name or len(cell_name) > 20:
            return False
        
        # æ˜ç¡®ä¸æ˜¯ç¼©å†™çš„å¸¸è§ç»†èƒç±»å‹ï¼ˆæ·»åŠ è¿™ä¸ªæ£€æŸ¥ï¼‰
        non_abbreviations = [
            'T cell', 'B cell', 'T cells', 'B cells',
            'NK cell', 'NK cells', 'dendritic cell', 'dendritic cells',
            'memory cell', 'naive cell', 'plasma cell', 'stem cell',
            'killer cell', 'helper cell', 'regulatory cell',
            'cytotoxic cell', 'effector cell', 'progenitor cell'
        ]
        
        if cell_name.lower() in [x.lower() for x in non_abbreviations]:
            return False
        
        # åŒ…å« "cell" æˆ– "cells" çš„é•¿çŸ­è¯­é€šå¸¸ä¸æ˜¯ç¼©å†™
        if 'cell' in cell_name.lower() and len(cell_name) > 6:
            return False
        
        # å¸¸è§çš„æ˜ç¡®ç¼©å†™æ¨¡å¼
        explicit_patterns = [
            r'^[A-Z]{2,5}$',                    # å…¨å¤§å†™å­—æ¯ï¼Œ2-5ä¸ªå­—ç¬¦ï¼šNK, DC, CTL
            r'^[A-Z][a-z]?[A-Z]+$',            # æ··åˆå¤§å°å†™ï¼šTh1, Tc, NK
            r'.*\+$',                           # ä»¥+ç»“å°¾: CD4+, CD8+
            r'.*-$',                            # ä»¥-ç»“å°¾: CD8-, Memory-
            r'^CD\d+[\+\-]?$',                 # CDå¼€å¤´+æ•°å­—: CD4, CD8+, CD25-
            r'^[A-Z]+\d+[\+\-]?$',             # å­—æ¯+æ•°å­—: Th1, Tc17, NK22+
            r'^[A-Z]{1,3}\d+[A-Za-z]*[\+\-]?$', # æ›´å¤æ‚çš„å­—æ¯æ•°å­—ç»„åˆ
        ]
        
        # å¤æ‚ç¼©å†™æ¨¡å¼ï¼ˆæ–°å¢ï¼‰
        complex_patterns = [
            r'^[A-Z]{1,3}\.[A-Za-z]+$',        # ç‚¹åˆ†éš”ï¼šTA.Early, NK.dim, DC.mature
            r'^[A-Z]{1,4}_[A-Za-z]+$',         # ä¸‹åˆ’çº¿åˆ†éš”ï¼šT_reg, NK_bright, DC_plasmacytoid
            r'^[A-Z]{1,3}[0-9]*\.[A-Z]{1,3}[0-9]*$', # ç‚¹åˆ†éš”å­—æ¯æ•°å­—ï¼šCD8.CM, Th1.Mem
            r'^[A-Za-z]{2,4}\.[0-9]+$',        # å­—æ¯ç‚¹æ•°å­—ï¼šTh.1, Tc.17
            r'^[A-Z]+/[A-Z]+$',                # æ–œæ åˆ†éš”ï¼šCD4/CD8, NK/NKT
            r'^[A-Z]{1,3}[0-9]+[a-z]+$',       # æ•°å­—åå°å†™ï¼šCD4lo, CD8hi, CD25dim
        ]
        
        # ç”Ÿç‰©å­¦ç‰¹å¼‚æ€§æ¨¡å¼
        bio_specific_patterns = [
            r'^[A-Z]{2,4}[\+\-]{1,3}$',        # å¤šä¸ª+/-ï¼šCD4++, CD8--, TCR+/-
            r'^[A-Z]{1,3}[0-9]*[a-z]{2,4}$',   # åç¼€æ¨¡å¼ï¼šCD4bright, CD8dim, NKbright
            r'^[A-Z]{1,4}[0-9]*[A-Z][a-z]*$',  # æ··åˆå¤§å°å†™ï¼šCD4Mem, CD8Eff, NKDim
            r'^p[A-Z]{1,3}[0-9]*[\+\-]?$',     # på¼€å¤´ï¼špDC, pTreg, pNK+
            r'^[A-Z]{1,3}SP$',                 # SPç»“å°¾ï¼šCD4SP, CD8SP (Single Positive)
            r'^[A-Z]{1,3}DP$',                 # DPç»“å°¾ï¼šCD4DP (Double Positive)
            r'^[A-Z]{1,3}DN$',                 # DNç»“å°¾ï¼šCD4DN (Double Negative)
        ]
        
        # ç»„ç»‡ç‰¹å¼‚æ€§ç¼©å†™
        tissue_specific_patterns = [
            r'^[A-Z]{2,3}C$',                  # ä»¥Cç»“å°¾çš„ç»†èƒï¼šHSC, MSC, NSC
            r'^[A-Z]{1,3}[0-9]*[A-Z]$',       # å­—æ¯æ•°å­—å­—æ¯ï¼šAT1, AT2, PT, DT
            r'^[A-Z]{2,4}[0-9]*$',             # çŸ­å­—æ¯æ•°å­—ç»„åˆï¼šOPC, OL, MG, AC
        ]
        
        # æ£€æŸ¥æ‰€æœ‰æ¨¡å¼
        all_patterns = (explicit_patterns + complex_patterns + 
                       bio_specific_patterns + tissue_specific_patterns)
        
        for pattern in all_patterns:
            if re.match(pattern, cell_name):
                return True
        
        # é•¿åº¦å’Œå­—ç¬¦ç»„åˆåˆ¤æ–­ï¼ˆæ”¹è¿›ï¼‰
        # çŸ­ä¸”åŒ…å«å¤§å†™å­—æ¯çš„å¯èƒ½æ˜¯ç¼©å†™ï¼Œä½†æ’é™¤å¸¸è§è¯ç»„
        if len(cell_name) <= 6:
            # åŒ…å«å¤§å†™å­—æ¯
            if any(c.isupper() for c in cell_name):
                # ä½†ä¸æ˜¯å¸¸è§çš„éç¼©å†™è¯ç»„
                if ' ' not in cell_name:  # å•ä¸ªè¯æ›´å¯èƒ½æ˜¯ç¼©å†™
                    return True
            # å…¨å°å†™ä½†å¾ˆçŸ­ä¸”å¸¸è§
            if cell_name.lower() in ['nk', 'dc', 'th', 'tc', 'treg', 'ctl']:
                return True
        
        # åŒ…å«ç‰¹æ®Šç¬¦å·é€šå¸¸æ˜¯ç¼©å†™
        special_chars = ['+', '-', '.', '_', '/', ':']
        if any(char in cell_name for char in special_chars):
            return True
        
        # æ•°å­—æ··åˆæ¨¡å¼
        if re.search(r'[A-Za-z]+[0-9]+', cell_name) or re.search(r'[0-9]+[A-Za-z]+', cell_name):
            return True
        
        # å¤šä¸ªè¿ç»­å¤§å†™å­—æ¯ï¼ˆä½†ä¸æ˜¯å¸¸è§è¯ç»„ï¼‰
        if re.search(r'[A-Z]{2,}', cell_name) and ' ' not in cell_name:
            return True
        
        # å¸¸è§ç»†èƒç±»å‹ç¼©å†™è¯å…¸æ£€æŸ¥
        common_abbreviations = {
            # å…ç–«ç»†èƒ
            'nk', 'dc', 'th', 'tc', 'treg', 'ctl', 'nkt', 'mait', 'ilc',
            'pdc', 'cdc', 'mdc', 'tam', 'tac', 'til', 'caf', 'msc',
            # ç¥ç»ç»†èƒ
            'opc', 'ol', 'mg', 'ac', 'pv', 'sst', 'vip', 'cck',
            # å…¶ä»–ç»„ç»‡
            'hsc', 'lsc', 'kc', 'pt', 'dt', 'ic', 'pc', 'pod',
            'at1', 'at2', 'am', 'club', 'hep', 'lsec',
            # å¹²ç»†èƒå’Œç¥–ç»†èƒ
            'esc', 'ipsc', 'npc', 'gpc', 'rpc', 'cpc',
            # ç™Œç»†èƒç›¸å…³
            'csc', 'ctc', 'caf', 'tam', 'tex', 'tn'
        }
        
        if cell_name.lower() in common_abbreviations:
            return True
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¸¸è§ç¼©å†™ä½œä¸ºå­ä¸²ï¼ˆä½†ä¸æ˜¯åŒ…å« cell çš„é•¿è¯ç»„ï¼‰
        for abbr in common_abbreviations:
            if abbr in cell_name.lower() and len(cell_name) <= 10 and 'cell' not in cell_name.lower():
                return True
        
        return False
    
    def test_abbreviation_detection(self, test_cases=None):
        """
        ğŸ§ª Test abbreviation detection with various examples
        
        Parameters
        ----------
        test_cases : dict, optional
            ğŸ“ Custom test cases {cell_name: expected_result}
        """
        if test_cases is None:
            # é¢„è®¾æµ‹è¯•ç”¨ä¾‹
            test_cases = {
                # åº”è¯¥è¢«è¯†åˆ«ä¸ºç¼©å†™çš„
                'NK': True,
                'DC': True, 
                'TA.Early': True,
                'CD4+': True,
                'CD8-': True,
                'Th1': True,
                'Treg': True,
                'pDC': True,
                'NK.dim': True,
                'T_reg': True,
                'CD8.CM': True,
                'Th.1': True,
                'CD4/CD8': True,
                'CD4lo': True,
                'CD8hi': True,
                'CD4++': True,
                'CD8SP': True,
                'HSC': True,
                'OPC': True,
                'AT1': True,
                'TAM': True,
                'CTL': True,
                'NKT': True,
                'pTreg': True,
                # ä¸åº”è¯¥è¢«è¯†åˆ«ä¸ºç¼©å†™çš„
                'T cell': False,
                'Natural killer cell': False,
                'Dendritic cell': False,
                'Regulatory T cell': False,
                'Memory T cell': False,
                'Naive B cell': False,
                'Activated macrophage': False,
                'Cytotoxic T lymphocyte': False,
                'Helper T cell': False,
                'Plasma cell': False,
            }
        
        print("Testing abbreviation detection...")
        print("=" * 60)
        
        correct = 0
        total = 0
        errors = []
        
        for cell_name, expected in test_cases.items():
            result = self._is_likely_abbreviation(cell_name)
            total += 1
            
            if result == expected:
                correct += 1
                status = "âœ“"
            else:
                errors.append((cell_name, expected, result))
                status = "âœ—"
            
            print(f"{status} {cell_name:<20} Expected: {expected}, Got: {result}")
        
        print("\n" + "=" * 60)
        print(f"Accuracy: {correct}/{total} ({correct/total*100:.1f}%)")
        
        if errors:
            print(f"\nâœ— Errors ({len(errors)}):")
            for cell_name, expected, got in errors:
                print(f"  - {cell_name}: expected {expected}, got {got}")
        else:
            print("All tests passed!")
        
        return correct / total
    
    def _call_llm_for_expansion(self, cell_name):
        """ğŸ¤– Call LLM for abbreviation expansion"""
        if self.llm_client is None:
            return None
        
        # Build context information
        context_parts = []
        
        if self.species and self.species != "human":
            context_parts.append(f"Species: {self.species}")
        
        if self.tissue_context:
            if isinstance(self.tissue_context, list):
                tissue_info = ", ".join(self.tissue_context)
            else:
                tissue_info = self.tissue_context
            context_parts.append(f"Tissue/Organ context: {tissue_info}")
        
        if self.study_context:
            context_parts.append(f"Study context: {self.study_context}")
        
        context_str = "\n".join(context_parts) if context_parts else ""
        
        # Get context-specific examples
        examples = self._get_context_specific_examples()
        
        prompt = f"""You are an expert in cell biology and immunology. Your task is to expand cell type abbreviations to their full, standard names.

{context_str}

Given the cell type abbreviation: "{cell_name}"

Please provide:
1. The most likely full name for this cell type
2. Alternative possible full names if there are multiple interpretations
3. Your confidence level (high/medium/low)

{examples}

Please respond in JSON format:
{{
    "primary_expansion": "most likely full name",
    "alternatives": ["alternative1", "alternative2"],
    "confidence": "high/medium/low",
    "reasoning": "brief explanation"
}}

Cell type abbreviation: {cell_name}"""

        # Initialize content variable
        content = None
        api_type = self.llm_config.get('api_type', 'unknown')
        
        try:
            if api_type in ["openai", "custom_openai", "doubao"]:
                # OpenAI API and compatible APIs
                response = self.llm_client.chat.completions.create(
                    model=self.llm_config['model'],
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.1,
                    max_tokens=300,
                    **self.llm_config.get('extra_params', {})
                )
                content = response.choices[0].message.content
                
            elif api_type == "anthropic":
                response = self.llm_client.messages.create(
                    model=self.llm_config['model'],
                    max_tokens=300,
                    temperature=0.1,
                    messages=[{"role": "user", "content": prompt}],
                    **self.llm_config.get('extra_params', {})
                )
                content = response.content[0].text
                
            elif api_type == "ollama":
                import requests
                response = requests.post(
                    f"{self.base_url}/api/generate",
                    json={
                        "model": self.llm_config['model'],
                        "prompt": prompt,
                        "stream": False,
                        "options": {"temperature": 0.1, **self.llm_config.get('extra_params', {})}
                    },
                    timeout=30
                )
                response.raise_for_status()
                content = response.json().get('response', '')
                
            elif api_type == "qwen":
                # é˜¿é‡Œäº‘é€šä¹‰åƒé—®
                import dashscope
                from dashscope import Generation
                response = dashscope.Generation.call(
                    model=self.llm_config['model'] or 'qwen-turbo',
                    messages=[{'role': 'user', 'content': prompt}],
                    temperature=0.1,
                    max_tokens=300,
                    **self.llm_config.get('extra_params', {})
                )
                if response.status_code == 200:
                    content = response.output.text
                else:
                    raise Exception(f"Qwen API error: {response.message}")
                    
            elif api_type == "ernie":
                # ç™¾åº¦æ–‡å¿ƒä¸€è¨€
                import ernie
                response = ernie.ChatCompletion.create(
                    model=self.llm_config['model'] or 'ernie-bot',
                    messages=[{'role': 'user', 'content': prompt}],
                    temperature=0.1,
                    **self.llm_config.get('extra_params', {})
                )
                content = response.get_result()
                
            elif api_type == "glm":
                # æ™ºè°±AI GLM
                import zhipuai
                response = zhipuai.model_api.invoke(
                    model=self.llm_config['model'] or 'chatglm_turbo',
                    prompt=[{'role': 'user', 'content': prompt}],
                    temperature=0.1,
                    **self.llm_config.get('extra_params', {})
                )
                if response['code'] == 200:
                    content = response['data']['choices'][0]['content']
                else:
                    raise Exception(f"GLM API error: {response.get('msg', 'Unknown error')}")
                    
            elif api_type == "spark":
                # è®¯é£æ˜Ÿç« (WebSocket API)
                content = self._call_spark_api(prompt)
                
            else:
                raise ValueError(f"Unsupported API type: {api_type}")
            
        except Exception as e:
            print(f"âœ— LLM call failed ({api_type}): {e}")
            return None
        
        # Check if content was successfully retrieved
        if content is None:
            print(f"âœ— No content received from {api_type} API")
            return None
        
        # Parse JSON response
        try:
            # Extract JSON part
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                return result
            else:
                # If no JSON format, try parsing text
                return {"primary_expansion": content.strip(), "confidence": "low"}
                
        except json.JSONDecodeError:
            return {"primary_expansion": content.strip(), "confidence": "low"}
    
    def _call_spark_api(self, prompt):
        """ğŸ”¥ Call iFlytek Spark API using WebSocket"""
        import requests
        import json
        import time
        import hashlib
        import hmac
        import base64
        from urllib.parse import urlencode
        
        # Spark API endpoint (using HTTP API instead of WebSocket for simplicity)
        url = "https://spark-api.xf-yun.com/v1.1/chat/completions"
        
        app_id = self.llm_config['app_id']
        api_key = self.llm_config['spark_api_key'] 
        api_secret = self.llm_config['api_secret']
        
        # Generate authentication
        timestamp = str(int(time.time()))
        signature_string = f"host: spark-api.xf-yun.com\ndate: {timestamp}\nGET /v1.1/chat/completions HTTP/1.1"
        signature = base64.b64encode(
            hmac.new(api_secret.encode(), signature_string.encode(), hashlib.sha256).digest()
        ).decode()
        
        authorization = f'api_key="{api_key}", algorithm="hmac-sha256", headers="host date request-line", signature="{signature}"'
        
        headers = {
            'Content-Type': 'application/json',
            'Authorization': authorization,
            'Date': timestamp,
            'Host': 'spark-api.xf-yun.com'
        }
        
        data = {
            "model": self.llm_config['model'] or "generalv3",
            "messages": [{"role": "user", "content": prompt}],
            "stream": False,
            "temperature": 0.1,
            "max_tokens": 300
        }
        
        try:
            response = requests.post(url, headers=headers, json=data, timeout=30)
            response.raise_for_status()
            result = response.json()
            return result['choices'][0]['message']['content']
        except Exception as e:
            # Fallback: return a simple HTTP API call
            print(f"âš ï¸  Spark WebSocket API failed, falling back to simple format: {e}")
            return f"Failed to expand {prompt.split('abbreviation: ')[-1]} via Spark API"
    
    def _get_context_specific_examples(self):
        """ğŸ¯ Generate context-specific examples"""
        base_examples = [
            "NK â†’ Natural killer cell",
            "DC â†’ Dendritic cell", 
            "Treg â†’ Regulatory T cell",
            "CD4+ â†’ CD4-positive T cell",
            "Th1 â†’ T helper 1 cell",
            "CTL â†’ Cytotoxic T lymphocyte"
        ]
        
        # Add tissue-specific examples
        tissue_examples = {
            "immune": [
                "NK â†’ Natural killer cell",
                "DC â†’ Dendritic cell",
                "Treg â†’ Regulatory T cell", 
                "pDC â†’ Plasmacytoid dendritic cell",
                "Th17 â†’ T helper 17 cell",
                "Tfh â†’ T follicular helper cell"
            ],
            "brain": [
                "OPC â†’ Oligodendrocyte precursor cell",
                "OL â†’ Oligodendrocyte",
                "AC â†’ Astrocyte",
                "MG â†’ Microglia",
                "PV+ â†’ Parvalbumin-positive interneuron"
            ],
            "liver": [
                "HSC â†’ Hepatic stellate cell",
                "KC â†’ Kupffer cell",
                "LSEC â†’ Liver sinusoidal endothelial cell",
                "Hep â†’ Hepatocyte"
            ],
            "kidney": [
                "PT â†’ Proximal tubule cell",
                "DT â†’ Distal tubule cell",
                "PC â†’ Principal cell",
                "IC â†’ Intercalated cell",
                "Pod â†’ Podocyte"
            ],
            "lung": [
                "AT1 â†’ Alveolar type 1 cell",
                "AT2 â†’ Alveolar type 2 cell",
                "AM â†’ Alveolar macrophage",
                "Club â†’ Club cell"
            ]
        }
        
        # Species-specific examples
        species_examples = {
            "mouse": [
                "mDC â†’ mouse Dendritic cell",
                "mTreg â†’ mouse Regulatory T cell"
            ],
            "rat": [
                "rNK â†’ rat Natural killer cell"
            ]
        }
        
        # Study context examples
        study_examples = {
            "cancer": [
                "CAF â†’ Cancer-associated fibroblast",
                "TAM â†’ Tumor-associated macrophage",
                "Tex â†’ Exhausted T cell",
                "TIL â†’ Tumor-infiltrating lymphocyte"
            ],
            "development": [
                "PSC â†’ Pluripotent stem cell",
                "NPC â†’ Neural progenitor cell",
                "HSC â†’ Hematopoietic stem cell"
            ]
        }
        
        # Select most relevant examples
        selected_examples = base_examples.copy()
        
        if self.tissue_context:
            for tissue_key in tissue_examples:
                if tissue_key.lower() in str(self.tissue_context).lower():
                    selected_examples.extend(tissue_examples[tissue_key][:3])
                    break
        
        if self.species and self.species in species_examples:
            selected_examples.extend(species_examples[self.species])
        
        if self.study_context:
            for study_key in study_examples:
                if study_key.lower() in self.study_context.lower():
                    selected_examples.extend(study_examples[study_key][:3])
                    break
        
        # Remove duplicates and limit quantity
        unique_examples = list(dict.fromkeys(selected_examples))[:8]
        
        return "Common examples:\n" + "\n".join(f"- {ex}" for ex in unique_examples)
    
    def expand_abbreviations(self, cell_names, force_expand=False, save_cache=True, 
                           tissue_context=None, species=None, study_context=None):
        """
        ğŸ”„ Expand cell type abbreviations
        
        Parameters
        ----------
        cell_names : list
            ğŸ“ List of cell names
        force_expand : bool
            ğŸ”’ Whether to force expand all names (not just abbreviations)
        save_cache : bool
            ğŸ’¾ Whether to save to cache
        tissue_context : str or list, optional
            ğŸ§¬ Temporary override tissue context information
        species : str, optional
            ğŸ­ Temporary override species information
        study_context : str, optional
            ğŸ”¬ Temporary override study context information
        
        Returns
        -------
        expanded_names : dict
            ğŸ“‹ Mapping from original names to expanded names
        """
        if self.llm_client is None:
            print("âœ— Please setup LLM API first using setup_llm_expansion()")
            return {name: name for name in cell_names}
        
        # Temporarily save original context
        original_tissue = self.tissue_context
        original_species = self.species
        original_study = self.study_context
        
        # Update with temporary context if provided
        if tissue_context is not None:
            self.tissue_context = tissue_context
        if species is not None:
            self.species = species
        if study_context is not None:
            self.study_context = study_context
        
        try:
            expanded_names = {}
            to_expand = []
            
            print("ğŸ” Analyzing cell names...")
            if self.tissue_context:
                print(f"ğŸ§¬ Using tissue context: {self.tissue_context}")
            if self.study_context:
                print(f"ğŸ”¬ Using study context: {self.study_context}")
            print(f"ğŸ­ Species: {self.species}")
            
            for cell_name in cell_names:
                # Check cache (with context-aware cache key)
                cache_key = self._get_cache_key(cell_name)
                if cache_key in self.abbreviation_cache:
                    expanded_names[cell_name] = self.abbreviation_cache[cache_key]['primary_expansion']
                    continue
                
                # Determine if expansion is needed
                if force_expand or self._is_likely_abbreviation(cell_name):
                    to_expand.append(cell_name)
                    print(f"  ğŸ”¤ Identified potential abbreviation: {cell_name}")
                else:
                    expanded_names[cell_name] = cell_name
            
            if not to_expand:
                print("âœ“ No abbreviations found to expand")
                return expanded_names
            
            print(f"\nğŸ¤– Expanding {len(to_expand)} abbreviations using LLM...")
            
            for i, cell_name in enumerate(to_expand):
                print(f"  ğŸ“ [{i+1}/{len(to_expand)}] Expanding: {cell_name}")
                
                result = self._call_llm_for_expansion(cell_name)
                
                if result and 'primary_expansion' in result:
                    expansion = result['primary_expansion']
                    expanded_names[cell_name] = expansion
                    
                    # Save to cache (using context-aware cache key)
                    cache_key = self._get_cache_key(cell_name)
                    self.abbreviation_cache[cache_key] = result
                    
                    print(f"    âœ“ â†’ {expansion} (Confidence: {result.get('confidence', 'unknown')})")
                    
                    if result.get('alternatives'):
                        print(f"    ğŸ’¡ Alternatives: {', '.join(result['alternatives'])}")
                else:
                    expanded_names[cell_name] = cell_name
                    print(f"    âœ— â†’ Expansion failed, keeping original")
            
            if save_cache:
                self._save_abbreviation_cache()
            
            return expanded_names
            
        finally:
            # Restore original context
            self.tissue_context = original_tissue
            self.species = original_species
            self.study_context = original_study
    
    def _get_cache_key(self, cell_name):
        """ğŸ”‘ Generate cache key with context information"""
        context_parts = [cell_name]
        
        if self.tissue_context:
            if isinstance(self.tissue_context, list):
                context_parts.append(f"tissue:{','.join(self.tissue_context)}")
            else:
                context_parts.append(f"tissue:{self.tissue_context}")
        
        if self.species and self.species != "human":
            context_parts.append(f"species:{self.species}")
        
        if self.study_context:
            context_parts.append(f"study:{self.study_context}")
        
        return "|".join(context_parts)
    
    def map_cells_with_expansion(self, cell_names, threshold=0.5, expand_abbreviations=True,
                               tissue_context=None, species=None, study_context=None):
        """
        ğŸ”„ First expand abbreviations, then perform ontology mapping
        
        Parameters
        ----------
        cell_names : list
            ğŸ“ List of cell names to map
        threshold : float
            ğŸ“Š Similarity threshold
        expand_abbreviations : bool
            ğŸ”„ Whether to enable abbreviation expansion
        tissue_context : str or list, optional
            ğŸ§¬ Tissue context information
        species : str, optional
            ğŸ­ Species information
        study_context : str, optional
            ğŸ”¬ Study context information
        
        Returns
        -------
        mapping_results : dict
            ğŸ“‹ Mapping results (including original and expanded name information)
        """
        if expand_abbreviations and self.llm_client is not None:
            print("ğŸ“ Step 1: Expanding abbreviations")
            expanded_names = self.expand_abbreviations(
                cell_names, 
                tissue_context=tissue_context,
                species=species, 
                study_context=study_context
            )
            
            print("\nğŸ¯ Step 2: Performing ontology mapping")
            expanded_cell_names = list(expanded_names.values())
            base_results = self.map_cells(expanded_cell_names, threshold)
            
            # Reorganize results with original name information
            mapping_results = {}
            for original_name in cell_names:
                expanded_name = expanded_names[original_name]
                if expanded_name in base_results:
                    result = base_results[expanded_name].copy()
                    result['original_name'] = original_name
                    result['expanded_name'] = expanded_name
                    result['was_expanded'] = (original_name != expanded_name)
                    mapping_results[original_name] = result
                else:
                    # This shouldn't happen, but as backup
                    mapping_results[original_name] = {
                        'best_match': 'Unknown',
                        'similarity': 0.0,
                        'confidence': 'low',
                        'original_name': original_name,
                        'expanded_name': expanded_name,
                        'was_expanded': (original_name != expanded_name),
                        'top3_matches': []
                    }
        else:
            print("ğŸ¯ Performing direct ontology mapping (abbreviation expansion disabled)")
            mapping_results = self.map_cells(cell_names, threshold)
            
            # Add expansion information
            for cell_name in mapping_results:
                mapping_results[cell_name]['original_name'] = cell_name
                mapping_results[cell_name]['expanded_name'] = cell_name
                mapping_results[cell_name]['was_expanded'] = False
        
        return mapping_results
    
    def map_adata_with_expansion(self, adata, cell_name_col=None, threshold=0.5, 
                                new_col_name='cell_ontology', expand_abbreviations=True,
                                tissue_context=None, species=None, study_context=None):
        """
        ğŸ§¬ Perform ontology mapping with abbreviation expansion on AnnData
        
        Parameters
        ----------
        adata : AnnData
            ğŸ“Š Single-cell data object
        cell_name_col : str, optional
            ğŸ“ Column name containing cell names
        threshold : float
            ğŸ“Š Similarity threshold
        new_col_name : str
            ğŸ·ï¸  New column name
        expand_abbreviations : bool
            ğŸ”„ Whether to enable abbreviation expansion
        tissue_context : str or list, optional
            ğŸ§¬ Tissue context information, e.g. "immune system", "brain", "liver"
        species : str, optional
            ğŸ­ Species information, e.g. "human", "mouse", "rat"
        study_context : str, optional
            ğŸ”¬ Study context information, e.g. "cancer", "development", "aging"
        
        Returns
        -------
        mapping_results : dict
            ğŸ“‹ Mapping results
        """
        # Get cell names
        if cell_name_col is None:
            cell_names = adata.obs.index.unique().tolist()
            cell_names_series = adata.obs.index.to_series()
            print(f"ğŸ“Š Using {len(cell_names)} unique cell names from index")
        else:
            cell_names = adata.obs[cell_name_col].unique().tolist()
            cell_names_series = adata.obs[cell_name_col]
            print(f"ğŸ“Š Using {len(cell_names)} unique cell names from column '{cell_name_col}'")
        
        # Perform mapping with expansion
        mapping_results = self.map_cells_with_expansion(
            cell_names, threshold, expand_abbreviations,
            tissue_context=tissue_context,
            species=species,
            study_context=study_context
        )
        
        # Apply to adata
        print("\nğŸ“ Applying mapping results to AnnData...")
        
        def get_best_match(cell_name):
            return mapping_results.get(cell_name, {}).get('best_match', 'Unknown')
        
        def get_similarity(cell_name):
            return mapping_results.get(cell_name, {}).get('similarity', 0.0)
        
        def get_confidence(cell_name):
            return mapping_results.get(cell_name, {}).get('confidence', 'low')
        
        def get_ontology_id(cell_name):
            return mapping_results.get(cell_name, {}).get('ontology_id', None)
        
        def get_cl_id(cell_name):
            return mapping_results.get(cell_name, {}).get('cl_id', None)
        
        def get_expanded_name(cell_name):
            return mapping_results.get(cell_name, {}).get('expanded_name', cell_name)
        
        def was_expanded(cell_name):
            return mapping_results.get(cell_name, {}).get('was_expanded', False)
        
        adata.obs[new_col_name] = cell_names_series.apply(get_best_match)
        adata.obs[f'{new_col_name}_similarity'] = cell_names_series.apply(get_similarity)
        adata.obs[f'{new_col_name}_confidence'] = cell_names_series.apply(get_confidence)
        adata.obs[f'{new_col_name}_ontology_id'] = cell_names_series.apply(get_ontology_id)
        adata.obs[f'{new_col_name}_cl_id'] = cell_names_series.apply(get_cl_id)
        adata.obs[f'{new_col_name}_expanded'] = cell_names_series.apply(get_expanded_name)
        adata.obs[f'{new_col_name}_was_expanded'] = cell_names_series.apply(was_expanded)
        
        # Statistics
        high_conf_count = sum(1 for r in mapping_results.values() if r['confidence'] == 'high')
        expanded_count = sum(1 for r in mapping_results.values() if r['was_expanded'])
        
        print(f"âœ“ Mapping completed:")
        print(f"  ğŸ“Š {high_conf_count}/{len(mapping_results)} cell names have high confidence mapping")
        print(f"  ğŸ”„ {expanded_count}/{len(mapping_results)} cell names underwent abbreviation expansion")
        
        return mapping_results
    
    def show_expansion_summary(self, mapping_results):
        """ğŸ“Š Show abbreviation expansion summary"""
        expanded_items = [
            (name, result) for name, result in mapping_results.items() 
            if result.get('was_expanded', False)
        ]
        
        if not expanded_items:
            print("â„¹ï¸  No abbreviation expansions performed")
            return
        
        print(f"\nğŸ“‹ Abbreviation Expansion Summary ({len(expanded_items)} items)")
        print("=" * 60)
        for name, result in expanded_items:
            print(f"ğŸ”¤ {name} â†’ {result['expanded_name']}")
            print(f"  ğŸ¯ Mapped to: {result['best_match']} (Similarity: {result['similarity']:.3f})")
            if name in self.abbreviation_cache:
                cache_info = self.abbreviation_cache[name]
                if cache_info.get('confidence'):
                    print(f"  ğŸ“Š Expansion confidence: {cache_info['confidence']}")
            print()
    
    def clear_abbreviation_cache(self):
        """ğŸ—‘ï¸  Clear abbreviation cache"""
        self.abbreviation_cache = {}
        if self.cache_file and os.path.exists(self.cache_file):
            os.remove(self.cache_file)
        print("âœ“ Abbreviation cache cleared")
    
    def _check_network_connection(self, timeout=5):
        """
        ğŸŒ Check network connectivity
        
        Parameters
        ----------
        timeout : int
            â±ï¸ Connection timeout in seconds
            
        Returns
        -------
        bool
            âœ“ True if network is available, False otherwise
        """
        import socket
        try:
            # Try to connect to Google DNS
            socket.create_connection(("8.8.8.8", 53), timeout)
            return True
        except OSError:
            try:
                # Try to connect to Baidu (for China users)
                socket.create_connection(("baidu.com", 80), timeout)
                return True
            except OSError:
                return False
    
    def _setup_hf_mirror(self):
        """
        ğŸª Setup HF-Mirror environment
        """
        import os
        # Set HF-Mirror endpoint
        os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
        print("ğŸ‡¨ğŸ‡³ Using HF-Mirror (hf-mirror.com) for faster downloads in China")
    
    def set_model(self, model_name, local_model_dir=None):
        """
        ğŸ¯ Set model name and local save directory
        
        Parameters
        ----------
        model_name : str
            ğŸ¤– Model name (e.g., 'all-mpnet-base-v2', 'sentence-transformers/all-MiniLM-L6-v2')
        local_model_dir : str, optional
            ğŸ“ Local directory to save the model (avoid default cache)
        """
        self.model_name = model_name
        self.local_model_dir = local_model_dir
        self.model = None  # Reset model to trigger reload
        
        print(f"ğŸ¯ Model set to: {model_name}")
        if local_model_dir:
            print(f"ğŸ“ Local save directory: {local_model_dir}")
        print("ğŸ’¡ Model will be downloaded when first used")
    
    def set_local_model(self, model_path):
        """
        ğŸ  Set local model path
        
        Parameters
        ----------
        model_path : str
            ğŸ“ Local model directory path
        """
        if not os.path.exists(model_path):
            raise ValueError(f"âœ— Model path does not exist: {model_path}")
        
        self.model_name = model_path
        self.model = None  # Reset model to trigger reload
        print(f"âœ“ Local model path set to: {model_path}")
        print("ğŸ’¡ Model will be loaded when first used")
    
    def _load_model(self):
        """ğŸ¤– Lazy load sentence transformer model with network detection and HF-Mirror support"""
        if self.model is None:
            from sentence_transformers import SentenceTransformer
            import os
            
            print(f"ğŸ”„ Loading model {self.model_name}...")
            
            try:
                # 1. æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°è·¯å¾„
                if os.path.exists(self.model_name):
                    print(f"ğŸ“ Loading local model from: {self.model_name}")
                    self.model = SentenceTransformer(self.model_name)
                    print(f"âœ“ Local model loaded successfully!")
                    return
                
                # 2. æ£€æŸ¥ç½‘ç»œè¿æ¥
                print("ğŸŒ Checking network connectivity...")
                has_network = self._check_network_connection()
                
                if not has_network:
                    raise ConnectionError("âœ— No network connection available")
                
                print("âœ“ Network connection available")
                
                # 3. è®¾ç½® HF-Mirror åŠ é€Ÿä¸‹è½½
                self._setup_hf_mirror()
                
                # 4. å‡†å¤‡ä¸‹è½½å‚æ•°
                download_kwargs = {}
                
                # å¦‚æœæŒ‡å®šäº†æœ¬åœ°ä¿å­˜ç›®å½•ï¼Œè®¾ç½® cache_folder
                if self.local_model_dir:
                    # ç¡®ä¿ç›®å½•å­˜åœ¨
                    os.makedirs(self.local_model_dir, exist_ok=True)
                    download_kwargs['cache_folder'] = self.local_model_dir
                    print(f"ğŸ“ Models will be saved to: {self.local_model_dir}")
                
                # 5. å°è¯•ä» HF-Mirror ä¸‹è½½æ¨¡å‹
                print(f"ğŸª Downloading model from HF-Mirror: {self.model_name}")
                
                self.model = SentenceTransformer(
                    self.model_name, 
                    **download_kwargs
                )
                
                print(f"âœ“ Model loaded successfully from HF-Mirror!")
                
                # å¦‚æœæŒ‡å®šäº†æœ¬åœ°ç›®å½•ï¼Œæ˜¾ç¤ºå®é™…ä¿å­˜ä½ç½®
                if self.local_model_dir:
                    model_path = os.path.join(self.local_model_dir, f"models--sentence-transformers--{self.model_name.replace('/', '--')}")
                    if os.path.exists(model_path):
                        print(f"ğŸ’¾ Model cached at: {model_path}")
                
            except Exception as e:
                print(f"âœ— Failed to load model from HF-Mirror: {e}")
                
                # 6. å›é€€åˆ°å®˜æ–¹ HuggingFace Hub
                print("ğŸ”„ Falling back to official HuggingFace Hub...")
                try:
                    # ç§»é™¤ HF-Mirror è®¾ç½®
                    if 'HF_ENDPOINT' in os.environ:
                        del os.environ['HF_ENDPOINT']
                    
                    download_kwargs = {}
                    if self.local_model_dir:
                        download_kwargs['cache_folder'] = self.local_model_dir
                    
                    self.model = SentenceTransformer(
                        self.model_name,
                        **download_kwargs
                    )
                    
                    print(f"âœ“ Model loaded successfully from official HuggingFace!")
                    
                except Exception as e2:
                    print(f"âœ— Failed to load model from official source: {e2}")
                    print(f"ğŸ’¡ Please check:")
                    print(f"   - Model name is correct: {self.model_name}")
                    print(f"   - Network connection is stable")
                    print(f"   - Sufficient disk space available")
                    if self.local_model_dir:
                        print(f"   - Directory permissions for: {self.local_model_dir}")
                    raise
    
    def create_ontology_resources(self, cl_obo_file, save_embeddings=True):
        """
        ğŸ”¨ Create ontology resources from OBO file
        
        Parameters
        ----------
        cl_obo_file : str
            ğŸ“„ Cell Ontology OBO file path
        save_embeddings : bool
            ğŸ’¾ Whether to save embeddings to file
        """
        self.output_path = Path(cl_obo_file).parent
        
        print("ğŸ“– Parsing ontology file...")
        with open(cl_obo_file) as f:
            graph = json.load(f)["graphs"][0]
        
        # Build ontology dictionary
        self.popv_dict = {}
        self.popv_dict["nodes"] = [
            entry for entry in graph["nodes"] 
            if entry["type"] == "CLASS" and entry.get("lbl", False)
        ]
        
        self.popv_dict["lbl_sentence"] = {
            entry["lbl"]: f"{entry['lbl']}: {entry.get('meta', {}).get('definition', {}).get('val', '')} {' '.join(entry.get('meta', {}).get('comments', []))}"
            for entry in self.popv_dict["nodes"]
        }
        
        self.popv_dict["id_2_lbl"] = {entry["id"]: entry["lbl"] for entry in self.popv_dict["nodes"]}
        self.popv_dict["lbl_2_id"] = {entry["lbl"]: entry["id"] for entry in self.popv_dict["nodes"]}
        
        self.popv_dict["edges"] = [
            i for i in graph["edges"]
            if i["sub"].split("/")[-1][0:2] == "CL" and i["obj"].split("/")[-1][0:2] == "CL" and i["pred"] == "is_a"
        ]
        
        self.popv_dict["ct_edges"] = [
            [self.popv_dict["id_2_lbl"][i["sub"]], self.popv_dict["id_2_lbl"][i["obj"]]] 
            for i in self.popv_dict["edges"]
        ]
        
        # Create embeddings
        print("ğŸ§  Creating NLP embeddings...")
        self._create_embeddings()
        
        # Save resources
        if save_embeddings:
            self.save_embeddings()
        
        self._save_ontology_files()
        print("âœ“ Ontology resources creation completed!")
    
    def _create_embeddings(self):
        """ğŸ§  Create ontology embeddings"""
        self._load_model()
        
        sentences = list(self.popv_dict["lbl_sentence"].values())
        labels = list(self.popv_dict["lbl_sentence"].keys())
        
        print(f"ğŸ”„ Encoding {len(sentences)} ontology labels...")
        sentence_embeddings = self.model.encode(sentences, show_progress_bar=True)
        
        self.ontology_embeddings = {}
        for label, embedding in zip(labels, sentence_embeddings):
            self.ontology_embeddings[label] = embedding
        
        self.ontology_labels = labels
    
    def save_embeddings(self, output_path=None):
        """ğŸ’¾ Save embeddings to file"""
        if output_path is None:
            output_path = self.output_path
        
        save_data = {
            'embeddings': self.ontology_embeddings,
            'labels': self.ontology_labels,
            'model_name': self.model_name,
            'popv_dict': getattr(self, 'popv_dict', None)  # Include popv_dict for ontology IDs
        }
        
        embeddings_file = os.path.join(output_path, "ontology_embeddings.pkl")
        with open(embeddings_file, "wb") as f:
            pickle.dump(save_data, f)
        
        print(f"ğŸ’¾ Embeddings saved to: {embeddings_file}")
        if save_data['popv_dict'] is not None:
            print(f"ğŸ“‹ Ontology mappings included: {len(save_data['popv_dict'].get('lbl_2_id', {}))} cell types")
    
    def load_embeddings(self, embeddings_path):
        """ğŸ“¥ Load embeddings from file"""
        with open(embeddings_path, "rb") as f:
            save_data = pickle.load(f)
        
        self.ontology_embeddings = save_data['embeddings']
        self.ontology_labels = save_data['labels']
        self.model_name = save_data.get('model_name', self.model_name)
        
        # Load popv_dict if available (for ontology IDs)
        if 'popv_dict' in save_data and save_data['popv_dict'] is not None:
            self.popv_dict = save_data['popv_dict']
            print(f"ğŸ“¥ Loaded embeddings for {len(self.ontology_labels)} ontology labels")
            print(f"ğŸ“‹ Ontology mappings loaded: {len(self.popv_dict.get('lbl_2_id', {}))} cell types")
        else:
            self.popv_dict = None
            print(f"ğŸ“¥ Loaded embeddings for {len(self.ontology_labels)} ontology labels")
            print("âš ï¸  No ontology ID mappings found in file (ontology_id will be None)")
            print("ğŸ’¡ Use create_ontology_resources() to generate complete ontology data with IDs")
    
    def load_ontology_mappings(self, popv_json_path):
        """
        ğŸ“‹ Load ontology ID mappings from cl_popv.json file
        
        Parameters
        ----------
        popv_json_path : str
            ğŸ“„ Path to cl_popv.json file
        
        Returns
        -------
        success : bool
            âœ“ True if loaded successfully
        """
        try:
            with open(popv_json_path, 'r', encoding='utf-8') as f:
                self.popv_dict = json.load(f)
            
            mapping_count = len(self.popv_dict.get('lbl_2_id', {}))
            print(f"âœ“ Loaded ontology mappings: {mapping_count} cell types")
            print("ğŸ“‹ Ontology IDs will now be available in mapping results")
            return True
            
        except FileNotFoundError:
            print(f"âœ— File not found: {popv_json_path}")
            print("ğŸ’¡ Make sure the cl_popv.json file exists")
            return False
        except json.JSONDecodeError as e:
            print(f"âœ— JSON decode error: {e}")
            return False
        except Exception as e:
            print(f"âœ— Failed to load ontology mappings: {e}")
            return False
    
    def check_ontology_status(self):
        """
        ğŸ” Check ontology data status and provide diagnostic information
        
        Returns
        -------
        status : dict
            ğŸ“Š Status information
        """
        status = {
            'embeddings_loaded': self.ontology_embeddings is not None,
            'labels_count': len(self.ontology_labels) if self.ontology_labels else 0,
            'popv_dict_loaded': self.popv_dict is not None,
            'ontology_mappings_count': 0,
            'can_provide_ontology_ids': False
        }
        
        if self.popv_dict and 'lbl_2_id' in self.popv_dict:
            status['ontology_mappings_count'] = len(self.popv_dict['lbl_2_id'])
            status['can_provide_ontology_ids'] = True
        
        print("ğŸ” === Ontology Status Diagnostic ===")
        print(f"ğŸ“Š Embeddings loaded: {'âœ“' if status['embeddings_loaded'] else 'âœ—'}")
        print(f"ğŸ“ Ontology labels: {status['labels_count']}")
        print(f"ğŸ“‹ Ontology mappings loaded: {'âœ“' if status['popv_dict_loaded'] else 'âœ—'}")
        print(f"ğŸ†” Ontology ID mappings: {status['ontology_mappings_count']}")
        print(f"ğŸ¯ Can provide ontology IDs: {'âœ“' if status['can_provide_ontology_ids'] else 'âœ—'}")
        
        if not status['can_provide_ontology_ids']:
            print("\nğŸ’¡ === Solutions to get Ontology IDs ===")
            print("1. ğŸ”¨ Create complete ontology resources:")
            print("   mapper.create_ontology_resources('cl.json')")
            print("2. ğŸ“‹ Load ontology mappings separately:")
            print("   mapper.load_ontology_mappings('cl_popv.json')")
            print("3. ğŸ”„ Re-save embeddings to include mappings:")
            print("   # After loading mappings, re-save embeddings")
            print("   mapper.save_embeddings()")
        
        return status
    
    def _save_ontology_files(self):
        """ğŸ’¾ Save other ontology files"""
        if self.output_path is None:
            return
        
        # Save JSON file
        with open(f"{self.output_path}/cl_popv.json", "w") as f:
            json.dump(self.popv_dict, f, indent=4)
        
        # Save edge information
        children_edge_celltype_df = pd.DataFrame(self.popv_dict["ct_edges"])
        children_edge_celltype_df.to_csv(
            f"{self.output_path}/cl.ontology", 
            sep="\t", header=False, index=False
        )
        
        # Save text format embeddings
        output_file = os.path.join(self.output_path, "cl.ontology.nlp.emb")
        with open(output_file, "w") as fout:
            for label, vec in self.ontology_embeddings.items():
                fout.write(label + "\t" + "\t".join(map(str, vec)) + "\n")
    
    def map_cells(self, cell_names, threshold=0.5):
        """
        ğŸ¯ Map cell names to ontology
        
        Parameters
        ----------
        cell_names : list
            ğŸ“ List of cell names to map
        threshold : float
            ğŸ“Š Similarity threshold
        
        Returns
        -------
        mapping_results : dict
            ğŸ“‹ Mapping results (now includes ontology IDs)
        """
        if self.ontology_embeddings is None:
            raise ValueError("âœ— Please load or create ontology embeddings first")
        
        self._load_model()
        
        print(f"ğŸ¯ Mapping {len(cell_names)} cell names...")
        
        # Encode cell names
        cell_embeddings = self.model.encode(cell_names, show_progress_bar=True)
        
        # Get ontology embedding matrix
        ontology_emb_matrix = np.array([
            self.ontology_embeddings[label] for label in self.ontology_labels
        ])
        
        # Calculate similarities
        similarities = cosine_similarity(cell_embeddings, ontology_emb_matrix)
        
        mapping_results = {}
        for i, cell_name in enumerate(cell_names):
            # Find best match
            best_match_idx = np.argmax(similarities[i])
            best_similarity = similarities[i][best_match_idx]
            best_match_label = self.ontology_labels[best_match_idx]
            
            # Get ontology ID information for best match
            ontology_info = self._get_ontology_id(best_match_label)
            
            # Get top 3 best matches with their IDs
            top3_indices = np.argsort(similarities[i])[-3:][::-1]
            top3_matches = []
            for idx in top3_indices:
                match_label = self.ontology_labels[idx]
                match_similarity = similarities[i][idx]
                match_ontology_info = self._get_ontology_id(match_label)
                top3_matches.append({
                    'label': match_label,
                    'similarity': match_similarity,
                    'ontology_id': match_ontology_info['ontology_id'],
                    'cl_id': match_ontology_info['cl_id']
                })
            
            mapping_results[cell_name] = {
                'best_match': best_match_label,
                'similarity': best_similarity,
                'confidence': 'high' if best_similarity > threshold else 'low',
                'ontology_id': ontology_info['ontology_id'],
                'cl_id': ontology_info['cl_id'],
                'top3_matches': top3_matches
            }
        
        return mapping_results
    
    def map_adata(self, adata, cell_name_col=None, threshold=0.5, new_col_name='cell_ontology'):
        """
        ğŸ§¬ Map cell names in AnnData object to ontology
        
        Parameters
        ----------
        adata : AnnData
            ğŸ“Š Single-cell data object
        cell_name_col : str, optional
            ğŸ“ Column name containing cell names, use index if None
        threshold : float
            ğŸ“Š Similarity threshold
        new_col_name : str
            ğŸ·ï¸  New column name
        
        Returns
        -------
        mapping_results : dict
            ğŸ“‹ Mapping results
        """
        # Get cell names
        if cell_name_col is None:
            cell_names = adata.obs.index.unique().tolist()
            cell_names_series = adata.obs.index.to_series()
            print(f"ğŸ“Š Using {len(cell_names)} unique cell names from index")
        else:
            cell_names = adata.obs[cell_name_col].unique().tolist()
            cell_names_series = adata.obs[cell_name_col]
            print(f"ğŸ“Š Using {len(cell_names)} unique cell names from column '{cell_name_col}'")
        
        # Perform mapping
        mapping_results = self.map_cells(cell_names, threshold)
        
        # Apply to adata
        print("ğŸ“ Applying mapping results to AnnData...")
        
        def get_best_match(cell_name):
            return mapping_results.get(cell_name, {}).get('best_match', 'Unknown')
        
        def get_similarity(cell_name):
            return mapping_results.get(cell_name, {}).get('similarity', 0.0)
        
        def get_confidence(cell_name):
            return mapping_results.get(cell_name, {}).get('confidence', 'low')
        
        def get_ontology_id(cell_name):
            return mapping_results.get(cell_name, {}).get('ontology_id', None)
        
        def get_cl_id(cell_name):
            return mapping_results.get(cell_name, {}).get('cl_id', None)
        
        def get_expanded_name(cell_name):
            return mapping_results.get(cell_name, {}).get('expanded_name', cell_name)
        
        def was_expanded(cell_name):
            return mapping_results.get(cell_name, {}).get('was_expanded', False)
        
        adata.obs[new_col_name] = cell_names_series.apply(get_best_match)
        adata.obs[f'{new_col_name}_similarity'] = cell_names_series.apply(get_similarity)
        adata.obs[f'{new_col_name}_confidence'] = cell_names_series.apply(get_confidence)
        adata.obs[f'{new_col_name}_ontology_id'] = cell_names_series.apply(get_ontology_id)
        adata.obs[f'{new_col_name}_cl_id'] = cell_names_series.apply(get_cl_id)
        adata.obs[f'{new_col_name}_expanded'] = cell_names_series.apply(get_expanded_name)
        adata.obs[f'{new_col_name}_was_expanded'] = cell_names_series.apply(was_expanded)
        
        # Statistics
        high_conf_count = sum(1 for r in mapping_results.values() if r['confidence'] == 'high')
        print(f"âœ“ Mapping completed: {high_conf_count}/{len(mapping_results)} cell names have high confidence mapping")
        
        return mapping_results
    
    def get_statistics(self, mapping_results):
        """ğŸ“Š Get mapping statistics"""
        total = len(mapping_results)
        high_conf = sum(1 for r in mapping_results.values() if r['confidence'] == 'high')
        low_conf = total - high_conf
        
        similarities = [r['similarity'] for r in mapping_results.values()]
        
        stats = {
            'total_mappings': total,
            'high_confidence': high_conf,
            'low_confidence': low_conf,
            'high_confidence_ratio': high_conf / total if total > 0 else 0,
            'mean_similarity': np.mean(similarities),
            'median_similarity': np.median(similarities),
            'min_similarity': np.min(similarities),
            'max_similarity': np.max(similarities)
        }
        
        return stats
    
    def print_mapping_summary_with_ids(self, mapping_results, top_n=10):
        """ğŸ“‹ Print mapping summary with ontology IDs"""
        stats = self.get_statistics(mapping_results)
        
        print("\n" + "="*60)
        print("MAPPING STATISTICS SUMMARY")
        print("="*60)
        print(f"Total mappings:\t\t{stats['total_mappings']}")
        print(f"High confidence:\t{stats['high_confidence']} ({stats['high_confidence_ratio']:.2%})")
        print(f"Low confidence:\t\t{stats['low_confidence']}")
        print(f"Average similarity:\t{stats['mean_similarity']:.3f}")
        print(f"Median similarity:\t{stats['median_similarity']:.3f}")
        
        print(f"\nTOP {top_n} MAPPING RESULTS")
        print("-" * 60)
        sorted_results = sorted(
            mapping_results.items(), 
            key=lambda x: x[1]['similarity'], 
            reverse=True
        )
        
        for i, (cell_name, result) in enumerate(sorted_results[:top_n]):
            conf_mark = "âœ“" if result['confidence'] == 'high' else "?"
            cl_id = result.get('cl_id', 'N/A')
            print(f"\n{i+1:2d}. [{conf_mark}] {cell_name}")
            print(f"     â†’ {result['best_match']}")
            print(f"     Similarity: {result['similarity']:.3f}")
            print(f"     CL ID: {cl_id}")
            if result.get('was_expanded', False):
                print(f"     Expanded from: {result.get('expanded_name', cell_name)}")
            print()
    
    def print_mapping_summary(self, mapping_results, top_n=10):
        """ğŸ“‹ Print mapping summary"""
        stats = self.get_statistics(mapping_results)
        
        print("\n" + "="*60)
        print("MAPPING STATISTICS SUMMARY")
        print("="*60)
        print(f"Total mappings:\t\t{stats['total_mappings']}")
        print(f"High confidence:\t{stats['high_confidence']} ({stats['high_confidence_ratio']:.2%})")
        print(f"Low confidence:\t\t{stats['low_confidence']}")
        print(f"Average similarity:\t{stats['mean_similarity']:.3f}")
        print(f"Median similarity:\t{stats['median_similarity']:.3f}")
        
        print(f"\nTOP {top_n} MAPPING RESULTS")
        print("-" * 60)
        sorted_results = sorted(
            mapping_results.items(), 
            key=lambda x: x[1]['similarity'], 
            reverse=True
        )
        
        for i, (cell_name, result) in enumerate(sorted_results[:top_n]):
            conf_mark = "âœ“" if result['confidence'] == 'high' else "?"
            print(f"{conf_mark} {cell_name} -> {result['best_match']} (Similarity: {result['similarity']:.3f})")
    
    def save_mapping_results(self, mapping_results, output_file):
        """ğŸ’¾ Save mapping results to file"""
        results_data = []
        
        for cell_name, result in mapping_results.items():
            row_data = {
                'cell_name': cell_name,
                'best_match': result['best_match'],
                'similarity': result['similarity'],
                'confidence': result['confidence'],
                'ontology_id': result.get('ontology_id', ''),
                'cl_id': result.get('cl_id', ''),
            }
            
            # Handle top3_matches (new structure with dictionaries)
            top3_matches = result.get('top3_matches', [])
            for i, match in enumerate(top3_matches[:3], 1):
                if isinstance(match, dict):
                    # New structure
                    row_data[f'top{i}_match'] = match.get('label', '')
                    row_data[f'top{i}_similarity'] = match.get('similarity', 0)
                    row_data[f'top{i}_ontology_id'] = match.get('ontology_id', '')
                    row_data[f'top{i}_cl_id'] = match.get('cl_id', '')
                else:
                    # Old structure (tuple)
                    row_data[f'top{i}_match'] = match[0] if len(match) > 0 else ''
                    row_data[f'top{i}_similarity'] = match[1] if len(match) > 1 else 0
                    row_data[f'top{i}_ontology_id'] = ''
                    row_data[f'top{i}_cl_id'] = ''
            
            # Fill missing top matches
            for i in range(len(top3_matches) + 1, 4):
                row_data[f'top{i}_match'] = ''
                row_data[f'top{i}_similarity'] = 0
                row_data[f'top{i}_ontology_id'] = ''
                row_data[f'top{i}_cl_id'] = ''
            
            results_data.append(row_data)
        
        results_df = pd.DataFrame(results_data)
        results_df.to_csv(output_file, index=False)
        print(f"Results saved to: {output_file}")
        print(f"Columns: {list(results_df.columns)}")
    
    def list_ontology_cells(self, max_display=50, return_all=False):
        """
        ğŸ“‹ List all cell types in the ontology
        
        Parameters
        ----------
        max_display : int
            ğŸ“Š Maximum number to display
        return_all : bool
            ğŸ“ Whether to return complete list
        
        Returns
        -------
        cell_types : list
            ğŸ“‹ List of cell types
        """
        if self.ontology_labels is None:
            raise ValueError("âœ— Please load or create ontology embeddings first")
        
        total_count = len(self.ontology_labels)
        print(f"ğŸ“Š Total {total_count} cell types in ontology")
        
        if max_display > 0:
            print(f"\nğŸ“‹ First {min(max_display, total_count)} cell types:")
            for i, cell_type in enumerate(self.ontology_labels[:max_display]):
                print(f"{i+1:3d}. {cell_type}")
            
            if total_count > max_display:
                print(f"... {total_count - max_display} more cell types")
                print("ğŸ’¡ Use return_all=True to get complete list")
        
        if return_all:
            return self.ontology_labels.copy()
        else:
            return self.ontology_labels[:max_display]
    
    def search_ontology_cells(self, keyword, case_sensitive=False, max_results=20):
        """
        ğŸ” Search cell types containing specific keywords in the ontology
        
        Parameters
        ----------
        keyword : str
            ğŸ”¤ Search keyword
        case_sensitive : bool
            ğŸ“ Whether case sensitive
        max_results : int
            ğŸ“Š Maximum number of results to return
        
        Returns
        -------
        matches : list
            ğŸ“‹ List of matching cell types
        """
        if self.ontology_labels is None:
            raise ValueError("âœ— Please load or create ontology embeddings first")
        
        if not case_sensitive:
            keyword = keyword.lower()
            search_labels = [label.lower() for label in self.ontology_labels]
        else:
            search_labels = self.ontology_labels
        
        matches = []
        original_matches = []
        
        for i, label in enumerate(search_labels):
            if keyword in label:
                matches.append(label)
                original_matches.append(self.ontology_labels[i])
        
        print(f"ğŸ” Found {len(matches)} cell types containing '{keyword}':")
        
        for i, match in enumerate(original_matches[:max_results]):
            print(f"{i+1:3d}. {match}")
        
        if len(matches) > max_results:
            print(f"... {len(matches) - max_results} more results")
        
        return original_matches[:max_results]
    
    def get_cell_info(self, cell_name):
        """
        â„¹ï¸  Get detailed information for specific cell type
        
        Parameters
        ----------
        cell_name : str
            ğŸ“ Cell type name
        
        Returns
        -------
        info : dict
            â„¹ï¸  Cell information dictionary
        """
        if self.ontology_labels is None:
            raise ValueError("âœ— Please load or create ontology embeddings first")
        
        if cell_name not in self.ontology_labels:
            print(f"âœ— Cell type not found: {cell_name}")
            # Try fuzzy matching
            similar = self.search_ontology_cells(cell_name, max_results=5)
            if similar:
                print("ğŸ’¡ Did you mean one of these:")
                for s in similar:
                    print(f"  - {s}")
            return None
        
        info = {'name': cell_name}
        
        # Add more information if popv_dict exists
        if self.popv_dict and 'lbl_sentence' in self.popv_dict:
            if cell_name in self.popv_dict['lbl_sentence']:
                info['description'] = self.popv_dict['lbl_sentence'][cell_name]
            
            if 'lbl_2_id' in self.popv_dict and cell_name in self.popv_dict['lbl_2_id']:
                info['ontology_id'] = self.popv_dict['lbl_2_id'][cell_name]
        
        # Display information
        print(f"\nâ„¹ï¸  === {cell_name} ===")
        if 'ontology_id' in info:
            print(f"ğŸ†” Ontology ID: {info['ontology_id']}")
        if 'description' in info:
            print(f"ğŸ“ Description: {info['description']}")
        
        return info
    
    def browse_ontology_by_category(self, categories=None, max_per_category=10):
        """
        ğŸ“‚ Browse ontology cell types by category
        
        Parameters
        ----------
        categories : list, optional
            ğŸ“ List of category keywords to view
        max_per_category : int
            ğŸ“Š Maximum number to display per category
        """
        if self.ontology_labels is None:
            raise ValueError("âœ— Please load or create ontology embeddings first")
        
        if categories is None:
            categories = [
                'T cell', 'B cell', 'NK cell', 'dendritic cell', 'macrophage',
                'neutrophil', 'eosinophil', 'basophil', 'monocyte', 'lymphocyte',
                'epithelial cell', 'endothelial cell', 'fibroblast', 'neuron',
                'stem cell', 'progenitor cell', 'cancer cell', 'tumor cell'
            ]
        
        print("ğŸ“‚ === Browse Ontology Cell Types by Category ===\n")
        
        for category in categories:
            matches = self.search_ontology_cells(category, max_results=max_per_category)
            if matches:
                print(f"\nğŸ·ï¸  ã€{category} relatedã€‘ (Showing top {len(matches)}):")
                for i, match in enumerate(matches):
                    print(f"  {i+1}. {match}")
            print("-" * 50)
    
    def get_ontology_statistics(self):
        """ğŸ“Š Get ontology statistics"""
        if self.ontology_labels is None:
            raise ValueError("âœ— Please load or create ontology embeddings first")
        
        total_cells = len(self.ontology_labels)
        
        # Analyze cell type name length distribution
        lengths = [len(label) for label in self.ontology_labels]
        
        # Count common words
        all_words = []
        for label in self.ontology_labels:
            words = label.lower().split()
            all_words.extend(words)
        
        from collections import Counter
        word_counts = Counter(all_words)
        common_words = word_counts.most_common(10)
        
        stats = {
            'total_cell_types': total_cells,
            'avg_name_length': np.mean(lengths),
            'min_name_length': np.min(lengths),
            'max_name_length': np.max(lengths),
            'common_words': common_words
        }
        
        print("ğŸ“Š === Ontology Statistics ===")
        print(f"ğŸ“ Total cell types: {stats['total_cell_types']}")
        print(f"ğŸ“ Average name length: {stats['avg_name_length']:.1f} characters")
        print(f"ğŸ“ Shortest name length: {stats['min_name_length']} characters")
        print(f"ğŸ“ Longest name length: {stats['max_name_length']} characters")
        print(f"\nğŸ”¤ Most common words:")
        for word, count in common_words:
            print(f"  {word}: {count} times")
        
        return stats
    
    def find_similar_cells(self, cell_name, top_k=10):
        """
        ğŸ” Find ontology cell types most similar to given cell name
        
        Parameters
        ----------
        cell_name : str
            ğŸ“ Input cell name
        top_k : int
            ğŸ“Š Return top k most similar results
        
        Returns
        -------
        similar_cells : list
            ğŸ“‹ Similar cell types and their similarities
        """
        if self.ontology_embeddings is None:
            raise ValueError("âœ— Please load or create ontology embeddings first")
        
        self._load_model()
        
        # Encode input cell name
        cell_embedding = self.model.encode([cell_name])
        
        # Get ontology embedding matrix
        ontology_emb_matrix = np.array([
            self.ontology_embeddings[label] for label in self.ontology_labels
        ])
        
        # Calculate similarities
        similarities = cosine_similarity(cell_embedding, ontology_emb_matrix)[0]
        
        # Get top-k most similar
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        similar_cells = [
            (self.ontology_labels[idx], similarities[idx]) 
            for idx in top_indices
        ]
        
        print(f"\nğŸ¯ Ontology cell types most similar to '{cell_name}':")
        for i, (label, sim) in enumerate(similar_cells):
            print(f"{i+1:2d}. {label:<40} (Similarity: {sim:.3f})")
        
        return similar_cells
    
    def download_model(self):
        """
        ğŸ“¥ Manually download and load the model
        
        Returns
        -------
        bool
            âœ“ True if successful, False otherwise
        """
        try:
            self._load_model()
            return True
        except Exception as e:
            print(f"âœ— Model download failed: {e}")
            return False
    
    def _extract_cl_id(self, ontology_id):
        """
        ğŸ†” Extract CL number from ontology ID
        
        Parameters
        ----------
        ontology_id : str
            ğŸ“ Full ontology ID like "http://purl.obolibrary.org/obo/CL_0000084"
            
        Returns
        -------
        cl_id : str
            ğŸ”¢ CL ID like "CL:0000084" or None if not found
        """
        if not ontology_id:
            return None
            
        try:
            # Extract CL number from URL format
            if "CL_" in ontology_id:
                cl_number = ontology_id.split("CL_")[-1]
                return f"CL:{cl_number}"
            # Handle other formats if needed
            elif "CL:" in ontology_id:
                return ontology_id.split("/")[-1]
            else:
                return None
        except:
            return None
    
    def _get_ontology_id(self, cell_label):
        """
        ğŸ”— Get ontology ID for a cell label
        
        Parameters
        ----------
        cell_label : str
            ğŸ“ Cell type label
            
        Returns
        -------
        ontology_info : dict
            ğŸ“‹ Dictionary with ontology_id and cl_id
        """
        if not self.popv_dict or 'lbl_2_id' not in self.popv_dict:
            return {'ontology_id': None, 'cl_id': None}
        
        ontology_id = self.popv_dict['lbl_2_id'].get(cell_label)
        cl_id = self._extract_cl_id(ontology_id)
        
        return {
            'ontology_id': ontology_id,
            'cl_id': cl_id
        }

# ğŸ› ï¸  Utility functions (maintaining backward compatibility)
def get_minified_adata(adata) -> AnnData:
    """ğŸ“¦ Return a minified AnnData."""
    adata = adata.copy()
    if hasattr(adata, 'raw') and adata.raw is not None:
        del adata.raw
    all_zeros = csr_matrix(adata.X.shape)
    X = all_zeros
    layers = {layer: all_zeros.copy() for layer in adata.layers}
    adata.X = X
    adata.layers = layers
    return adata

def majority_vote(x):
    """ğŸ—³ï¸  Majority voting function"""
    a, b = np.unique(x, return_counts=True)
    return a[np.argmax(b)]

def majority_count(x):
    """ğŸ”¢ Majority counting function"""
    a, b = np.unique(x, return_counts=True)
    return np.max(b)

def download_cl(output_dir="new_ontology", filename="cl.json"):
    """
    ğŸ“¥ Download Cell Ontology file from multiple sources with automatic fallback
    
    This is a standalone function that downloads cl.json from multiple sources:
    1. Official OBO Library (direct JSON)
    2. OSS Mirror for Chinese users (ZIP format)
    3. Google Drive backup (ZIP format)
    
    Parameters
    ----------
    output_dir : str, optional
        Directory to save the file (default: "new_ontology")
    filename : str, optional
        Output filename (default: "cl.json")
        
    Returns
    -------
    success : bool
        True if download successful, False otherwise
    file_path : str or None
        Path to downloaded file if successful
    
    Examples
    --------
    >>> success, file_path = download_cl()
    >>> if success:
    ...     print(f"Downloaded to: {file_path}")
    
    >>> success, file_path = download_cl("my_data", "cell_ontology.json")
    """
    import requests
    import zipfile
    import json
    import socket
    import os
    from pathlib import Path
    
    def check_network_connection(timeout=5):
        """Check if network is available"""
        try:
            socket.create_connection(("8.8.8.8", 53), timeout)
            return True
        except OSError:
            try:
                socket.create_connection(("baidu.com", 80), timeout)
                return True
            except OSError:
                return False
    
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, filename)
    
    # Define download sources
    sources = [
        {
            'name': 'Official OBO Library',
            'url': 'http://purl.obolibrary.org/obo/cl/cl.json',
            'is_zip': False,
            'description': 'Direct download from official Cell Ontology'
        },
        {
            'name': 'OSS Mirror (China)',
            'url': 'https://starlit.oss-cn-beijing.aliyuncs.com/single/cl.json.zip',
            'is_zip': True,
            'description': 'Fast mirror for Chinese users'
        },
        {
            'name': 'Google Drive Backup',
            'url': 'https://drive.google.com/uc?export=download&id=1niokr5INjWFVjiXHfdCoWioh0ZEYCPkv',
            'is_zip': True,
            'description': 'Google Drive backup copy'
        }
    ]
    
    print(f"Downloading Cell Ontology to: {output_path}")
    print("=" * 60)
    
    for i, source in enumerate(sources, 1):
        print(f"\n[{i}/{len(sources)}] Trying {source['name']}...")
        print(f"    URL: {source['url']}")
        print(f"    Description: {source['description']}")
        
        try:
            # Check network connectivity
            if not check_network_connection():
                print("    âœ— No network connection available")
                continue
            
            # Download file
            print("    â†’ Downloading...")
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(source['url'], headers=headers, stream=True, timeout=30)
            response.raise_for_status()
            
            # Determine temporary file path
            if source['is_zip']:
                temp_file = os.path.join(output_dir, f"temp_{filename}.zip")
            else:
                temp_file = output_path
            
            # Save downloaded content with tqdm progress bar
            total_size = int(response.headers.get('content-length', 0))
            
            if total_size > 0:
                # Use tqdm progress bar
                with open(temp_file, 'wb') as f:
                    with tqdm(total=total_size, unit='B', unit_scale=True, 
                            desc="    Progress", ncols=80, leave=False) as pbar:
                        for chunk in response.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)
                                pbar.update(len(chunk))
            else:
                # Fallback to simple progress display
                downloaded = 0
                with open(temp_file, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                            downloaded += len(chunk)
                            if total_size > 0:
                                progress = (downloaded / total_size) * 100
                                print(f"\r    â†’ Progress: {progress:.1f}%", end='', flush=True)
                if total_size == 0:
                    print()  # New line after progress
            
            # Get final file size for display
            actual_size = os.path.getsize(temp_file) if os.path.exists(temp_file) else 0
            print(f"    â†’ Downloaded: {actual_size / (1024*1024):.2f} MB")
            
            # Handle zip files
            if source['is_zip']:
                print("    â†’ Extracting ZIP file...")
                try:
                    with zipfile.ZipFile(temp_file, 'r') as zip_ref:
                        # Look for cl.json in the zip
                        json_files = [f for f in zip_ref.namelist() if f.endswith('.json')]
                        if not json_files:
                            raise ValueError("No JSON file found in ZIP archive")
                        
                        # Extract the first JSON file found
                        json_file = json_files[0]
                        print(f"    â†’ Extracting: {json_file}")
                        
                        # Extract with progress if possible
                        # Get file info for progress
                        file_info = zip_ref.getinfo(json_file)
                        extract_size = file_info.file_size
                        
                        with zip_ref.open(json_file) as source_file, \
                             open(output_path, 'wb') as target_file:
                            
                            if extract_size > 0:
                                with tqdm(total=extract_size, unit='B', unit_scale=True,
                                        desc="    Extracting", ncols=80, leave=False) as pbar:
                                    while True:
                                        chunk = source_file.read(8192)
                                        if not chunk:
                                            break
                                        target_file.write(chunk)
                                        pbar.update(len(chunk))
                            else:
                                # No size info, just copy without progress
                                target_file.write(source_file.read())
                        
                        print(f"    â†’ Extracted to: {output_path}")
                    
                    # Remove temporary zip file
                    os.remove(temp_file)
                    
                except Exception as e:
                    print(f"    âœ— ZIP extraction failed: {e}")
                    if os.path.exists(temp_file):
                        os.remove(temp_file)
                    continue
            
            # Verify the downloaded file
            if not os.path.exists(output_path):
                print("    âœ— Output file not found after download")
                continue
            
            file_size = os.path.getsize(output_path)
            if file_size < 1024:  # Less than 1KB is probably an error
                print(f"    âœ— Downloaded file too small: {file_size} bytes")
                os.remove(output_path)
                continue
            
            # Try to validate JSON structure
            try:
                with open(output_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # Basic validation - check if it looks like an ontology file
                if 'graphs' not in data and 'nodes' not in data:
                    print("    âœ— File doesn't appear to be a valid ontology file")
                    os.remove(output_path)
                    continue
                
                print(f"    âœ“ File validation successful")
                
            except Exception as e:
                print(f"    âœ— JSON validation failed: {e}")
                os.remove(output_path)
                continue
            
            print(f"    âœ“ Successfully downloaded from {source['name']}")
            print(f"    File saved to: {output_path}")
            print(f"    File size: {file_size / (1024*1024):.2f} MB")
            
            return True, output_path
            
        except requests.exceptions.RequestException as e:
            print(f"    âœ— Network error: {e}")
            continue
            
        except Exception as e:
            print(f"    âœ— Download failed: {e}")
            continue
    
    print(f"\nâœ— All download sources failed")
    print("Suggestions:")
    print("   - Check your internet connection")
    print("   - Try again later")  
    print("   - Download manually and place in the output directory")
    
    return False, None

# ğŸš€ ================== Usage Examples ==================
"""
ğŸ’¡ Examples using HF-Mirror, custom local directories, and ontology IDs:

# 1. ğŸ“¥ Download Cell Ontology file (standalone function)
from omicverse.single._cellmatch import download_cl

# Basic download
success, file_path = download_cl()
if success:
    print(f"Downloaded to: {file_path}")

# Custom directory and filename
success, file_path = download_cl("my_ontology", "cell_ontology.json")

# 2. ğŸ”§ Basic setup with custom model directory
mapper = CellOntologyMapper(
    model_name="all-mpnet-base-v2",
    local_model_dir="./my_models"  # ğŸ“ Custom save directory
)

# 3. ğŸŒ Network detection and HF-Mirror download
mapper = CellOntologyMapper()
mapper.set_model(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    local_model_dir="/Users/your_username/ai_models"
)

# Manually download model
success = mapper.download_model()
if success:
    print("âœ“ Model ready for use!")

# 4. ğŸ§¬ Create ontology resources from downloaded file
# First download the ontology file
success, cl_file = download_cl("new_ontology")
if success:
    # Then create resources
    mapper.create_ontology_resources(cl_file, save_embeddings=True)

# Or create ontology resources from existing OBO file
mapper.create_ontology_resources("cl.obo.json", save_embeddings=True)

# 5. ğŸ¯ Map cell names with ontology IDs
cell_names = ["NK", "TA.Early", "CD4+", "Dendritic cell"]
mapping_results = mapper.map_cells(cell_names, threshold=0.5)

# Results now include ontology IDs:
for cell_name, result in mapping_results.items():
    print(f"ğŸ” {cell_name}")
    print(f"  â¤ Best match: {result['best_match']}")
    print(f"  ğŸ†” Ontology ID: {result['ontology_id']}")
    print(f"  ğŸ”¢ CL ID: {result['cl_id']}")
    print(f"  ğŸ“Š Similarity: {result['similarity']:.3f}")
    print(f"  ğŸ¯ Confidence: {result['confidence']}")
    print(f"  ğŸ“‹ Top 3 matches:")
    for i, match in enumerate(result['top3_matches'], 1):
        print(f"    {i}. {match['label']} (CL: {match['cl_id']}, Sim: {match['similarity']:.3f})")

# 6. ğŸ¤– Setup LLM expansion with context
mapper.setup_llm_expansion(
    api_type="openai",
    api_key="your_api_key",
    tissue_context="immune system",
    species="human",
    study_context="cancer"
)

# 7. ğŸ“Š Map AnnData with abbreviation expansion and ontology IDs
mapping_results = mapper.map_adata_with_expansion(
    adata, 
    cell_name_col="cell_type",
    new_col_name="cell_ontology",
    expand_abbreviations=True,
    tissue_context="immune system"
)

# AnnData now contains these columns:
# - cell_ontology: Best match cell type
# - cell_ontology_similarity: Similarity score
# - cell_ontology_confidence: Mapping confidence
# - cell_ontology_ontology_id: Full ontology ID
# - cell_ontology_cl_id: CL ID (e.g., "CL:0000084")
# - cell_ontology_expanded: Expanded cell name (if abbreviation)
# - cell_ontology_was_expanded: Whether abbreviation expansion was performed

# 8. ğŸ’¾ Save detailed results with ontology IDs
mapper.save_mapping_results(mapping_results, "cell_mapping_with_ids.csv")

# CSV will include columns:
# cell_name, best_match, similarity, confidence, ontology_id, cl_id,
# top1_match, top1_similarity, top1_ontology_id, top1_cl_id,
# top2_match, top2_similarity, top2_ontology_id, top2_cl_id,
# top3_match, top3_similarity, top3_ontology_id, top3_cl_id

# 9. ğŸ” Get detailed cell information with ontology ID
cell_info = mapper.get_cell_info("Natural killer cell")
print(f"Cell: {cell_info['name']}")
print(f"Ontology ID: {cell_info.get('ontology_id', 'N/A')}")
print(f"Description: {cell_info.get('description', 'N/A')}")

# ğŸ‡¨ğŸ‡³ =================== å›½äº§å¤§æ¨¡å‹ä½¿ç”¨ç¤ºä¾‹ ===================

# 10A. ğŸ¤– é˜¿é‡Œäº‘é€šä¹‰åƒé—® (DashScope)
# mapper.setup_llm_expansion(
#     api_type="qwen",
#     api_key="your_dashscope_api_key",  # æˆ–è®¾ç½®ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY
#     model="qwen-turbo",  # å¯é€‰: qwen-plus, qwen-max
#     tissue_context="immune system",
#     species="human",
#     study_context="cancer",
#     extra_params={"top_p": 0.8}  # å¯é€‰çš„é¢å¤–å‚æ•°
# )

# 10B. ğŸ¤– ç™¾åº¦æ–‡å¿ƒä¸€è¨€ (ERNIE)
# mapper.setup_llm_expansion(
#     api_type="ernie",
#     api_key="your_ernie_access_token",  # æˆ– "access_key:secret_key" æ ¼å¼
#     model="ernie-bot",  # å¯é€‰: ernie-bot-turbo, ernie-bot-4
#     tissue_context="brain",
#     species="mouse",
#     study_context="development"
# )

# 10C. ğŸ¤– æ™ºè°±AI GLM
# mapper.setup_llm_expansion(
#     api_type="glm",
#     api_key="your_zhipuai_api_key",  # æˆ–è®¾ç½®ç¯å¢ƒå˜é‡ ZHIPUAI_API_KEY
#     model="chatglm_turbo",  # å¯é€‰: chatglm_pro, chatglm_std
#     tissue_context="liver",
#     species="human"
# )

# 10D. ğŸ¤– è®¯é£æ˜Ÿç« (iFlytek Spark)
# mapper.setup_llm_expansion(
#     api_type="spark",
#     api_key="app_id:api_key:api_secret",  # ä¸‰ä¸ªå‚æ•°ç”¨å†’å·åˆ†éš”
#     model="generalv3",  # å¯é€‰: general, generalv2
#     tissue_context="lung",
#     study_context="cancer"
# )

# 10E. ğŸ¤– å­—èŠ‚è·³åŠ¨è±†åŒ… (Doubao/ç«å±±å¼•æ“)
# mapper.setup_llm_expansion(
#     api_type="doubao", 
#     api_key="your_doubao_api_key",
#     model="doubao-pro-4k",  # æˆ–å…¶ä»–æ¨¡å‹åç§°
#     base_url="https://ark.cn-beijing.volces.com/api/v3",  # å¯é€‰ï¼Œæœ‰é»˜è®¤å€¼
#     tissue_context="kidney",
#     study_context="aging"
# )

# 10F. ğŸ¤– è‡ªå®šä¹‰OpenAIå…¼å®¹APIï¼ˆå¦‚vLLMéƒ¨ç½²çš„æ¨¡å‹ï¼‰
# mapper.setup_llm_expansion(
#     api_type="custom_openai",
#     api_key="your_custom_api_key",  # å¯é€‰ï¼Œæ ¹æ®APIè¦æ±‚
#     model="your_model_name",
#     base_url="http://your-server:8000/v1",  # å¿…éœ€ï¼è‡ªå®šä¹‰APIåœ°å€
#     tissue_context="immune system",
#     extra_params={"stop": ["\n\n"]}  # å¯é€‰çš„é¢å¤–å‚æ•°
# )

# ğŸŒ =================== è‡ªå®šä¹‰base_urlå¢å¼ºåŠŸèƒ½ ===================

# 11A. ğŸŒ ä½¿ç”¨è‡ªå®šä¹‰base_urlçš„OpenAI API (å¦‚Azure OpenAI)
# mapper.setup_llm_expansion(
#     api_type="openai",
#     api_key="your_azure_api_key",
#     model="gpt-35-turbo",  # Azureæ¨¡å‹åç§°
#     base_url="https://your-resource.openai.azure.com/openai/deployments/your-deployment/",
#     extra_params={"api_version": "2023-05-15"}  # Azureç‰¹å®šå‚æ•°
# )

# 11B. ğŸŒ ç§æœ‰éƒ¨ç½²çš„OpenAIå…¼å®¹æœåŠ¡
# mapper.setup_llm_expansion(
#     api_type="custom_openai",
#     api_key="sk-xxx",  # ä½ çš„ç§æœ‰æœåŠ¡APIå¯†é’¥
#     model="llama2-7b-chat",  # ç§æœ‰æœåŠ¡ä¸­çš„æ¨¡å‹å
#     base_url="https://your-private-llm-api.com/v1",
#     tissue_context="brain",
#     extra_params={"temperature": 0.1, "max_tokens": 500}
# )

# 11C. ğŸŒ ä½¿ç”¨ä»£ç†çš„å›½å¤–æ¨¡å‹æœåŠ¡
# mapper.setup_llm_expansion(
#     api_type="openai",
#     api_key="your_api_key",
#     model="gpt-4",
#     base_url="https://api.openai-proxy.com/v1",  # ä»£ç†æœåŠ¡åœ°å€
#     tissue_context="immune system"
# )

# Features:
# âœ“ Standalone download function with multiple fallback sources
# âœ“ Automatic network detection
# ğŸª HF-Mirror acceleration for Chinese users  
# ğŸ“ Custom model save directory (no default cache)
# ğŸ”„ Automatic fallback to official HuggingFace
# ğŸ¤– LLM-powered abbreviation expansion
# ğŸ§¬ Context-aware cell type mapping
# ğŸ†” Full ontology ID support (including CL numbers)
# ğŸ“Š Comprehensive mapping results with top matches
# ğŸ’¾ Enhanced CSV export with all ontology information
# ğŸ“¥ ZIP file handling for compressed downloads
# ğŸ‡¨ğŸ‡³ Chinese domestic LLM support (é€šä¹‰åƒé—®, æ–‡å¿ƒä¸€è¨€, æ™ºè°±GLM, è®¯é£æ˜Ÿç«, è±†åŒ…)
# ğŸŒ Enhanced custom base_url support for private deployments

# ğŸ”§ =================== Ontology ID é—®é¢˜è§£å†³æ–¹æ¡ˆ ===================

# å¦‚æœé‡åˆ° ontology_id å…¨æ˜¯ None çš„é—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•ï¼š

# æ–¹æ³•1: æ£€æŸ¥æœ¬ä½“æ•°æ®çŠ¶æ€
# mapper.check_ontology_status()

# æ–¹æ³•2: ä»å®Œæ•´çš„æœ¬ä½“æ–‡ä»¶åˆ›å»ºèµ„æº
# success, cl_file = download_cl("new_ontology")
# if success:
#     mapper.create_ontology_resources(cl_file, save_embeddings=True)

# æ–¹æ³•3: å•ç‹¬åŠ è½½æœ¬ä½“IDæ˜ å°„ï¼ˆå¦‚æœæœ‰cl_popv.jsonæ–‡ä»¶ï¼‰
# mapper.load_ontology_mappings("new_ontology/cl_popv.json")

# æ–¹æ³•4: åŠ è½½embeddingsåå†åŠ è½½æ˜ å°„
# mapper.load_embeddings("ontology_embeddings.pkl")
# mapper.load_ontology_mappings("cl_popv.json")  # è¡¥å……åŠ è½½IDæ˜ å°„

# æ–¹æ³•5: é‡æ–°ä¿å­˜embeddingsä»¥åŒ…å«IDæ˜ å°„
# # å…ˆåŠ è½½å®Œæ•´æ•°æ®
# mapper.create_ontology_resources("cl.json")
# # é‡æ–°ä¿å­˜embeddingsï¼ˆç°åœ¨åŒ…å«IDæ˜ å°„ï¼‰
# mapper.save_embeddings()
"""