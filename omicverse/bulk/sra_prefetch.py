# sra_prefetch.py Prefetchæ­¥éª¤å·¥å‚
from __future__ import annotations
import os, time, subprocess, logging
import threading
from pathlib import Path
from typing import Callable, Optional, Sequence
try:
    from tqdm.auto import tqdm 
except Exception:
    from tqdm import tqdm
    
from concurrent.futures import ThreadPoolExecutor, as_completed
try:
    from .sra_tools import (get_sra_metadata, human_readable_speed, estimate_remaining_time, record_to_log, get_downloaded_file_size, run_prefetch_with_progress)
except ImportError:
    from sra_tools import (get_sra_metadata, human_readable_speed, estimate_remaining_time, record_to_log, get_downloaded_file_size, run_prefetch_with_progress)
# ---------- å·¥å…·å‡½æ•°ï¼ˆæŒ‰ä½ çš„å®ç°/å‘½åä¿ç•™ï¼‰ ----------
def find_sra_file(srr_id: str, output_root: Path) -> Path | None:
    """
    åœ¨ä¸€ç³»åˆ—å€™é€‰ä½ç½®æŸ¥æ‰¾ {srr_id}.sraï¼š
      1) output_root/SRR/SRR.sra
      2) output_root/SRR.sra
      3) srapath è¿”å›çš„ç¼“å­˜è·¯å¾„
      4) å¸¸è§æœ¬åœ°ç¼“å­˜ç›®å½• (~/.ncbi/public/sra, ~/ncbi/public/sra)
    æ‰¾åˆ°å°±è¿”å› Pathï¼Œå¦åˆ™ Noneã€‚
    """
    candidates = [
        output_root / srr_id / f"{srr_id}.sra",
        output_root / f"{srr_id}.sra",
    ]
    for p in candidates:
        if p.exists():
            return p

    # 3) srapath
    try:
        cp = subprocess.run(["srapath", srr_id],
                            capture_output=True, text=True, check=True)
        for line in cp.stdout.splitlines():
            p = Path(line.strip())
            if p.exists():
                return p
    except Exception:
        pass

    # 4) å¸¸è§ç¼“å­˜æ ¹
    home = Path.home()
    for root in [home/".ncbi/public/sra", home/"ncbi/public/sra"]:
        p = root / f"{srr_id}.sra"
        if p.exists():
            return p
    return None
def ensure_link_at(output_path: Path, real_file: Path, logger=None, prefer="symlink") -> Path:
    """
    ç¡®ä¿åœ¨ output_path æœ‰ä¸€ä¸ªå¯ç”¨çš„æ–‡ä»¶æŒ‡å‘ real_fileï¼š
      - ä¼˜å…ˆåˆ›å»ºç¬¦å·é“¾æ¥ï¼›ä¸æ”¯æŒæ—¶é€€å›ç¡¬é“¾æ¥/å¤åˆ¶
    è¿”å› output_path
    """
    output_path.parent.mkdir(parents=True, exist_ok=True)
    if output_path.exists():
        # å·²æœ‰å°±ç›´æ¥ç”¨
        return output_path

    try:
        if prefer == "symlink":
            output_path.symlink_to(real_file)
        elif prefer == "hardlink":
            os.link(real_file, output_path)
        else:
            shutil.copy2(real_file, output_path)
        if logger:
            logger.info(f"[LINK] {output_path} -> {real_file}")
    except Exception as e:
        # å…¼å®¹ä¸æ”¯æŒè½¯é“¾æ¥çš„ç¯å¢ƒï¼šé€€å›æ‹·è´
        if logger:
            logger.warning(f"[LINK] failed ({e}), fallback to copy")
        shutil.copy2(real_file, output_path)
    return output_path
    
def _make_local_logger(name: str, level=logging.INFO) -> logging.Logger:
    log_dir = Path("log")
    log_dir.mkdir(parents=True, exist_ok=True) 
    logger = logging.getLogger(f"sraprefetch.{name}")
    if not logger.handlers:
        logger.setLevel(level)
        fh = logging.FileHandler(log_dir/f"{name}.log", mode="a")
        sh = logging.StreamHandler()
        fmt = logging.Formatter(f"[%(asctime)s] [{name}] %(levelname)s: %(message)s")
        for h in (fh, sh):
            h.setFormatter(fmt)
            logger.addHandler(h)
    return logger

   ################################################
  #                                              #
 #      å…¼å®¹Alignment Classä¸PipLineå·¥å‚         #
#                                             #
##############################################

# ---------- For Alignment Class ----------
def _monitor_file_size(path: Path, pbar: tqdm, stop_event: threading.Event, interval: float = 1.0):
    """å¤–éƒ¨ç›‘æ§æ–‡ä»¶å¤§å°å¹¶åˆ·æ–° tqdm"""
    last = 0
    while not stop_event.is_set():
        try:
            size = path.stat().st_size if path.exists() else 0
            inc = max(0, size - last)
            if inc:
                pbar.update(inc)
                last = size
        except Exception:
            pass
        time.sleep(interval)
    # æ”¶å°¾å†åˆ·ä¸€æ¬¡
    try:
        size = path.stat().st_size if path.exists() else last
        if size > last:
            pbar.update(size - last)
    except Exception:
        pass
    pbar.close()
    
def prefetch_batch(
    srr_list: Sequence[str], 
    out_root: str | Path, 
    threads: int = 4, # åŒæ—¶ä¸‹è½½çš„ SRR æ•°
    retries: int = 3, 
    link_mode: str = "symlink",
    cache_root: str | Path = "sra_cache",
):
    """
    Adapter to integrate with Alignment.prefetch().
    For each SRR:
      1) call run_prefetch_with_progress(srr, logger, retries, output_root="sra_cache")
      2) locate the real .sra in the cache
      3) place a link/copy at <out_root>/<SRR>/<SRR>.sra
      å¹¶å‘ä¸‹è½½ SRAï¼šå¯¹ srr_list ä¸­çš„æ ·æœ¬å¹¶è¡Œæ‰§è¡Œ prefetchã€‚
      threads æ§åˆ¶â€œåŒæ—¶ä¸‹è½½ä»»åŠ¡æ•°â€
      ä¸‹è½½ååœ¨ cache_root ä¸­æ‰¾åˆ° .sraï¼Œå†é“¾æ¥/å¤åˆ¶åˆ° out_root/<SRR>/<SRR>.sra
    è¿”å›ï¼šæŒ‰è¾“å…¥é¡ºåºå¯¹é½çš„ç›®æ ‡ .sra è·¯å¾„åˆ—è¡¨
    """
    out_root = Path(out_root)
    cache_root = out_root 
    out_root.mkdir(parents=True, exist_ok=True)
    
    logger = _make_local_logger("prefetch")
    # å•ä¸ªä»»åŠ¡
    def _worker(pos: int, srr: str) -> tuple[str, Path]:
        
        # ç›‘æ§çš„ç›®æ ‡æ–‡ä»¶ï¼ˆä¸ä½ åŸå‡½æ•°ä¿æŒä¸€è‡´çš„è½ç›˜ä½ç½®ï¼‰
        srr_dir = cache_root / srr
        srr_dir.mkdir(parents=True, exist_ok=True)
        sra_file = srr_dir / f"{srr}.sra"

        '''# å°è¯•ä½¿ç”¨å¤šè¡Œè¿›åº¦æ¡ä½ç½®ï¼›ä¸æ”¯æŒæ—¶è‡ªåŠ¨é™çº§
        pbar_kwargs = dict(
            desc=f"ğŸ“¥ {srr}",
            total=None,  # è‹¥èƒ½æ‹¿åˆ°é¢„ä¼°å¤§å°å¯æ›¿æ¢ä¸ºå…·ä½“å€¼
            unit="B", unit_scale=True, unit_divisor=1024,
            dynamic_ncols=True, leave=False, mininterval=0.5
        )
        try:
            pbar = tqdm(position=pos, **pbar_kwargs)
        except TypeError:
            pbar = tqdm(**pbar_kwargs)
            
        stop_evt = threading.Event()
        mon = threading.Thread(target=_monitor_file_size, args=(sra_file, pbar, stop_evt), daemon=True)
        mon.start()
        '''
        # ä¸‹è½½
        ok = run_prefetch_with_progress(
            srr_id=srr, logger=logger, retries=retries, output_root=str(cache_root)
        )
        # åœæ­¢ç›‘æ§å¹¶æ”¶å°¾
        #stop_evt.set()
        #mon.join()
        
        if not ok:
            raise RuntimeError(f"Prefetch failed for {srr}")

        real = find_sra_file(srr_id=srr, output_root=cache_root)
        if not real or not Path(real).exists():
            raise RuntimeError(f"Prefetch done but cannot locate .sra for {srr}")

        dest = out_root / srr / f"{srr}.sra"
        dest.parent.mkdir(parents=True, exist_ok=True)
        ensure_link_at(Path(real), dest, prefer=link_mode, logger=logger)
        return srr, dest

    results_map: dict[str, Path] = {}
    failures: list[tuple[str, Exception]] = []

    # Multiple Download 
    with ThreadPoolExecutor(max_workers=int(threads)) as pool:
        fut_map = {pool.submit(_worker, pos,srr): srr for pos, srr in enumerate(srr_list)}
        for fut in as_completed(fut_map):
            srr = fut_map[fut]
            try:
                srr_id, dest_path = fut.result()
                results_map[srr_id] = dest_path
                logger.info(f"[prefetch] OK: {srr_id} -> {dest_path}")
            except Exception as e:
                failures.append((srr, e))
                logger.error(f"[prefetch] FAIL: {srr}: {e}")

    if failures:
        failed = ", ".join([f"{srr}({repr(err)})" for srr, err in failures])
        raise RuntimeError(f"Prefetch failed for {len(failures)} samples: {failed}")
    # ä¿æŒä¸è¾“å…¥é¡ºåºä¸€è‡´
    return [results_map[s] for s in srr_list if s in results_map]

# ---------- â€œæ­¥éª¤å·¥å‚â€ï¼šéœ€è¦æ—¶åœ¨ç¼–æ’æ¨¡å—é‡Œè°ƒç”¨ ----------
def make_prefetch_step(output_pattern: str = "sra_cache/{SRR}/{SRR}.sra", validation=None):
    def _safe_valid(fs: list[str]) -> bool:
        # fs åªä¼šæœ‰ä¸€ä¸ªï¼šæœŸæœ›è·¯å¾„ï¼Œä½†æˆ‘ä»¬ç”¨ find_sra_file æ¥å…œåº•
        try:
            f = Path(fs[0])
            if f.exists() and f.stat().st_size > 1_000_000:
                return True
            # ç”¨å®šä½å‡½æ•°æ£€æŸ¥çœŸå®ç¼“å­˜ä½ç½®
            real = find_sra_file(f.stem, f.parent.parent)  # stem=SRRxxxxxx, parent.parent=output_root
            return bool(real and real.exists() and real.stat().st_size > 1_000_000)
        except Exception:
            return False

    return {
        "name": "prefetch",
        "command": run_prefetch_with_progress,
        "outputs": [output_pattern],
        "validation": validation or _safe_valid,
    }