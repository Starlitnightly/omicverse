# omicverse/bulk/alignment.py
# Contributor: Zhi Luo

"""
Alignment pipeline for bulk RNA-seq in OmicVerse.

This class is a thin, composable wrapper around existing step modules in your repo:
- sra_prefetch / sra_fasterq   : SRA download & conversion
- qc_fastp / qc_tools          : FASTQ QC & trimming
- count_step / count_tools     : gene-level counting via featureCounts
- (optional) your aligner step : HISAT2/STAR wrapper if available
"""
from __future__ import annotations

import os
import json
import re
import logging
import subprocess
import threading
import time
from dataclasses import dataclass, field
from pathlib import Path
from typing import Iterable, Literal, Sequence, Tuple, Optional, Dict, Any, List, Union

# Import your existing step modules (assumes they are importable in the package env)
# If your modules live at project root, adjust relative imports accordingly.
try:
    from . import geo_meta_fetcher as _geo
except Exception:
    import geo_meta_fetcher as _geo

try:
    from . import entrez_direct as _ed
except Exception:
    import entrez_direct as _ed

try:
    from . import sra_prefetch as _sra_prefetch
except Exception:
    import sra_prefetch as _sra_prefetch
    
try:
    from . import sra_fasterq as _sra_fasterq
except Exception:
    import sra_fasterq as _sra_fasterq

try:
    from . import qc_fastp as _qc_fastp
except Exception:
    import qc_fastp as _qc_fastp

try:
    from . import star_step as _star_step
except Exception:
    import star_step as _star_step

try:
    from . import count_step as _count_step
except Exception:
    import count_step as _count_step

try:
    from . import tools_check as _tools_check
except Exception:
    import tools_check as _tools_check

try:
    from .iseq_handler import ISeqHandler
except Exception:
    from iseq_handler import ISeqHandler

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class DownloadProgressBar:
    """å®æ—¶ä¸‹è½½è¿›åº¦æ¡æ˜¾ç¤ºç±»"""

    def __init__(self, total_size=None, desc="Downloading"):
        self.total_size = total_size
        self.desc = desc
        self.current_size = 0
        self.start_time = time.time()
        self.last_update = self.start_time
        self.last_size = 0
        self.active = True
        self.lock = threading.Lock()

    def update(self, current_size):
        """æ›´æ–°è¿›åº¦"""
        with self.lock:
            self.current_size = current_size
            current_time = time.time()

            # æ¯0.5ç§’æ›´æ–°ä¸€æ¬¡æ˜¾ç¤ºï¼Œé¿å…è¿‡äºé¢‘ç¹
            if current_time - self.last_update >= 0.5:
                self._display_progress(current_time)
                self.last_update = current_time
                self.last_size = current_size

    def _display_progress(self, current_time):
        """æ˜¾ç¤ºè¿›åº¦æ¡"""
        if not self.active:
            return

        elapsed = current_time - self.start_time
        if elapsed == 0:
            return

        # è®¡ç®—é€Ÿåº¦ (bytes/second)
        speed = (self.current_size - self.last_size) / (current_time - self.last_update) if current_time > self.last_update else 0

        # æ ¼å¼åŒ–æ–‡ä»¶å¤§å°
        current_str = self._format_size(self.current_size)
        speed_str = self._format_size(speed) + "/s"

        if self.total_size and self.total_size > 0:
            # å¦‚æœæœ‰æ€»å¤§å°ï¼Œæ˜¾ç¤ºç™¾åˆ†æ¯”è¿›åº¦æ¡
            percentage = (self.current_size / self.total_size) * 100
            bar_length = 40
            filled_length = int(bar_length * self.current_size // self.total_size)
            bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
            total_str = self._format_size(self.total_size)
            eta = (self.total_size - self.current_size) / speed if speed > 0 else 0
            eta_str = self._format_time(eta)

            print(f'\r{self.desc}: {percentage:.1f}%|{bar}| {current_str}/{total_str} [{speed_str}, ETA: {eta_str}]', end='', flush=True)
        else:
            # å¦‚æœæ²¡æœ‰æ€»å¤§å°ï¼Œåªæ˜¾ç¤ºå½“å‰å¤§å°å’Œé€Ÿåº¦
            print(f'\r{self.desc}: {current_str} [{speed_str}]', end='', flush=True)

    def _format_size(self, size):
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        if size == 0:
            return "0B"

        units = ['B', 'KB', 'MB', 'GB', 'TB']
        unit_index = 0
        size_float = float(size)

        while size_float >= 1024 and unit_index < len(units) - 1:
            size_float /= 1024
            unit_index += 1

        if unit_index == 0:
            return f"{int(size_float)}{units[unit_index]}"
        else:
            return f"{size_float:.1f}{units[unit_index]}"

    def _format_time(self, seconds):
        """æ ¼å¼åŒ–æ—¶é—´"""
        if seconds < 60:
            return f"{int(seconds)}s"
        elif seconds < 3600:
            minutes = int(seconds // 60)
            secs = int(seconds % 60)
            return f"{minutes}m{secs}s"
        else:
            hours = int(seconds // 3600)
            minutes = int((seconds % 3600) // 60)
            return f"{hours}h{minutes}m"

    def finish(self):
        """å®Œæˆè¿›åº¦æ¡"""
        with self.lock:
            self.active = False
            # æ˜¾ç¤ºæœ€ç»ˆçŠ¶æ€
            total_time = time.time() - self.start_time
            total_str = self._format_size(self.current_size)
            avg_speed = self.current_size / total_time if total_time > 0 else 0
            avg_speed_str = self._format_size(avg_speed) + "/s"

            if self.total_size and self.current_size >= self.total_size:
                print(f'\r{self.desc}: 100%|{"â–ˆ" * 40}| {total_str}/{total_str} [{avg_speed_str}, Total: {self._format_time(total_time)}]')
            else:
                print(f'\r{self.desc}: {total_str} [{avg_speed_str}, Total: {self._format_time(total_time)}]')
            print()  # æ¢è¡Œ

    def stop(self):
        """åœæ­¢è¿›åº¦æ¡"""
        self.active = False


@dataclass
class AlignmentConfig:
    # IO roots
    work_root: Path = Path("work")
    meta_root: Path = field(init=False)
    prefetch_root: Path = field(init=False)
    fasterq_root: Path = field(init=False)
    qc_root: Path = field(init=False)
    align_root: Path = field(init=False)     # placeholder if you later add HISAT2/STAR
    counts_root: Path = field(init=False)

    star_index_root: Path = field(init=False)        # ç´¢å¼•æ ¹ç›®å½•ï¼ˆå’Œ star_tools è®¾å®šä¸€è‡´ï¼‰
    star_align_root: Path  = field(init=False)   # STAR è¾“å‡ºæ ¹ç›®å½•

    # Basic prefetch configuration (no mirror switching)
    prefetch_config: Dict[str, Any] = field(default_factory=lambda: {
        "enabled": True,                    # Enable prefetch
        "location": "ncbi",                 # Default location: ncbi
        "transport": "fasp",                # Default transport: fasp
        "retries": 3                        # Number of retry attempts
    })

    def __post_init__(self):
        
        if not isinstance(self.work_root, Path):
            self.work_root = Path(self.work_root)
        self.meta_root = self.work_root / "meta"
        self.prefetch_root = self.work_root / "prefetch"
        self.fasterq_root = self.work_root / "fasterq"
        self.qc_root = self.work_root / "fastp"
        self.align_root = self.work_root / "align" # other align methodé¢„ç•™ä½
        self.counts_root = self.work_root / "counts"
        self.star_align_root = self.work_root / "star"
        self.star_index_root = self.work_root / "index"

        # Automatically enable iseq if download_method is set to "iseq"
        if self.download_method == "iseq":
            self.iseq_enabled = True
    

    # Resources
    threads: int = 16
    memory: str = "8G"
    genome: Literal["human", "mouse", "custom"] = "human"
    gtf: Optional[Path] = None                 # if provided, overrides genome GTF discovery

    # Behavior
    gzip_fastq: bool = True
    fastp_enabled: bool = True
    simple_counts: bool = True                 # only gene_id,count
    download_method: str = "prefetch"          # "prefetch" or "iseq"

    # Metadata
    by: Literal["auto","srr","accession","sample"] = "auto"

    # iSeq settings
    iseq_enabled: bool = False
    iseq_sample_prefix: str = "Sample"
    iseq_sample_pattern: str = "auto"

    # iseq download options
    iseq_gzip: bool = True                    # Download FASTQ files in gzip format
    iseq_aspera: bool = False                 # Use Aspera for download
    iseq_database: str = "sra"                # Database: ena, sra
    iseq_protocol: str = "ftp"                # Protocol: ftp, https
    iseq_parallel: int = 4                    # Number of parallel downloads
    iseq_threads: int = 8                     # Threads for conversion/processing


class Alignment:
    """
    A cohesive, user-facing API for the bulk RNA-seq alignment pipeline.
    Each step delegates to your existing step modules, preserving their behavior,
    while unifying inputs/outputs and logging.
    """

    def __init__(self, config: AlignmentConfig | None = None):
        self.cfg = config or AlignmentConfig()
        # normalize paths
        for p in [
            self.cfg.work_root, self.cfg.meta_root,self.cfg.prefetch_root, self.cfg.fasterq_root,
            self.cfg.qc_root, self.cfg.align_root, self.cfg.counts_root,self.cfg.star_index_root,self.cfg.star_align_root
        ]:
            Path(p).mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–iSeqå¤„ç†å™¨
        self.iseq_handler = ISeqHandler(
            sample_id_pattern=self.cfg.iseq_sample_pattern,
            paired_end=True,  # é»˜è®¤åŒç«¯
            validate_files=True
        )

     # ---------- Fetch Metadata ----------
    def fetch_metadata(
        self,
        accession: str,
        meta_dir: Optional[Path] = None,
        out_dir: Optional[Path] = None,
        organism_filter: Optional[str] = None,   # ä¾‹å¦‚ "Homo sapiens"
        layout_filter: Optional[str] = None,     # "PAIRED" / "SINGLE"
    ):
        """
        ç»™ä¸€ä¸ª GEO accessionï¼ˆGSE/GSMï¼‰ï¼ŒæŠ“å– SOFTâ†’ä¿å­˜ meta JSONï¼Œ
        å†èµ° EDirect ç”Ÿæˆ RunInfo CSVï¼Œå¹¶è¿”å› SRR åˆ—è¡¨ä¸è·¯å¾„ã€‚
        """
        _tools_check.check()
        # 1) ç›®å½•è®¾ç½®
        meta_root = Path(meta_dir) if meta_dir else (Path(self.cfg.work_root) / "meta")
        #sra_meta_root = Path(out_dir) if out_dir else (Path(self.cfg.work_root) / "meta")
        meta_root.mkdir(parents=True, exist_ok=True)
        #sra_meta_root.mkdir(parents=True, exist_ok=True)
    
        # 2) ç”Ÿæˆ/æ›´æ–° JSON metadataï¼ˆæ³¨æ„æ˜¯ out_dir å‚æ•°ï¼‰
        _geo.geo_accession_to_meta_json(accession, out_dir=str(meta_root))
    
        # 3) ç”Ÿæˆ RunInfo CSVï¼ˆæ³¨æ„æ˜¯ accession + meta_dir/out_dirï¼‰
        info = _ed.gse_meta_to_runinfo_csv(
            accession=accession,
            meta_dir=str(meta_root),
            out_dir=str(meta_root),
            organism_filter=organism_filter,
            layout_filter=layout_filter,
        )
        runinfo_csv = Path(info["csv"]) if info.get("csv") else None
    
        # 4) æå– SRR åˆ—è¡¨
        srrs: List[str] = []
        if runinfo_csv and runinfo_csv.exists():
            import csv
            with runinfo_csv.open("r", newline="") as f:
                reader = csv.DictReader(f)
                for row in reader:
                    for key in ("Run", "acc", "Run Accession", "RunAccession"):
                        if key in row and row[key]:
                            srrs.append(row[key].strip())
                            break
            # å»é‡ä¿åº
            seen = set()
            srrs = [x for x in srrs if not (x in seen or seen.add(x))]
    
        # 5) å¯é€‰ï¼šç»“æ„åŒ–å­—æ®µ
        try:
            meta_struct = _geo.parse_geo_soft_to_struct(_geo.fetch_geo_text(accession))
        except Exception:
            meta_struct = None
    
        return {
            "meta_json": meta_root / f"{accession}_meta.json",
            "runinfo_csv": runinfo_csv,
            "srr_list": srrs,
            "edirect_info": info,   # åŒ…å« term_used/rows ç­‰
            "meta_struct": meta_struct,
        }

    def _parse_progress_line(self, line: str, progress_bar: DownloadProgressBar):
        """è§£æiseqè¾“å‡ºè¡Œçš„è¿›åº¦ä¿¡æ¯"""
        try:
            line_lower = line.lower()

            # åŒ¹é…å¸¸è§çš„è¿›åº¦æ ¼å¼
            # æ ¼å¼1: "45.2MB/125.8MB (36%)" æˆ– "45.2MB / 125.8MB (36%)"
            progress_pattern = r'(\d+(?:\.\d+)?)\s*([KMGT]?B)\s*/\s*(\d+(?:\.\d+)?)\s*([KMGT]?B)\s*\((\d+)%\)'
            match = re.search(progress_pattern, line, re.IGNORECASE)

            if match:
                current_size_str, current_unit, total_size_str, total_unit, percentage = match.groups()

                # è½¬æ¢ä¸ºå­—èŠ‚
                current_bytes = self._parse_size_to_bytes(float(current_size_str), current_unit)
                total_bytes = self._parse_size_to_bytes(float(total_size_str), total_unit)

                # æ›´æ–°è¿›åº¦æ¡
                progress_bar.total_size = total_bytes
                progress_bar.update(current_bytes)
                return

            # æ ¼å¼2: "Downloading: 45.2MB of 125.8MB"
            download_pattern = r'(\d+(?:\.\d+)?)\s*([KMGT]?B)\s+of\s+(\d+(?:\.\d+)?)\s*([KMGT]?B)'
            match = re.search(download_pattern, line, re.IGNORECASE)

            if match:
                current_size_str, current_unit, total_size_str, total_unit = match.groups()

                current_bytes = self._parse_size_to_bytes(float(current_size_str), current_unit)
                total_bytes = self._parse_size_to_bytes(float(total_size_str), total_unit)

                progress_bar.total_size = total_bytes
                progress_bar.update(current_bytes)
                return

            # æ ¼å¼3: "45.2MB downloaded" æˆ– "Downloaded: 45.2MB"
            downloaded_pattern = r'(\d+(?:\.\d+)?)\s*([KMGT]?B)\s*(?:downloaded|download)'
            match = re.search(downloaded_pattern, line, re.IGNORECASE)

            if match:
                size_str, unit = match.groups()
                current_bytes = self._parse_size_to_bytes(float(size_str), unit)
                progress_bar.update(current_bytes)
                return

            # æ ¼å¼4: ç™¾åˆ†æ¯”æ ¼å¼ "36%" æˆ– "Progress: 36%"
            percent_pattern = r'(\d+)%'
            match = re.search(percent_pattern, line)

            if match and progress_bar.total_size:
                percentage = int(match.group(1))
                current_bytes = int(progress_bar.total_size * percentage / 100)
                progress_bar.update(current_bytes)
                return

        except Exception as e:
            # è§£æå¤±è´¥æ—¶ä¸å½±å“ä¸»ç¨‹åº
            logger.debug(f"Failed to parse progress line '{line}': {e}")

    def _monitor_file_sizes(self, out_dir: Path, srr_list: Sequence[str], progress_bar: DownloadProgressBar):
        """ç›‘æ§ä¸‹è½½ç›®å½•ä¸­çš„æ–‡ä»¶å¤§å°å˜åŒ–"""
        try:
            logger.debug("Starting file size monitoring...")
            start_time = time.time()
            monitored_files = {}
            last_total_size = 0

            while progress_bar.active:
                current_time = time.time()

                # æ¯2ç§’æ£€æŸ¥ä¸€æ¬¡æ–‡ä»¶å¤§å°
                if current_time - start_time >= 2:
                    total_size = 0
                    found_files = 0

                    # æ£€æŸ¥æ¯ä¸ªSRRå¯¹åº”çš„æ–‡ä»¶
                    for srr in srr_list:
                        # æŸ¥æ‰¾å¯èƒ½çš„æ–‡ä»¶æ¨¡å¼
                        patterns = [
                            f"{srr}*.fastq*",      # FASTQæ–‡ä»¶ï¼ˆå¯èƒ½å‹ç¼©ï¼‰
                            f"{srr}*.fq*",         # çŸ­æ‰©å±•å
                            f"{srr}*.sra*",        # SRAæ ¼å¼
                            f"{srr}*.sralite"      # SRA liteæ ¼å¼
                        ]

                        srr_size = 0
                        for pattern in patterns:
                            files = list(out_dir.glob(pattern))
                            for file_path in files:
                                if file_path.is_file():
                                    try:
                                        file_size = file_path.stat().st_size
                                        srr_size += file_size

                                        # è®°å½•æ–‡ä»¶å¤§å°å˜åŒ–
                                        file_key = str(file_path)
                                        if file_key not in monitored_files:
                                            monitored_files[file_key] = file_size
                                            logger.debug(f"Found new file: {file_path.name} ({self._format_size(file_size)})")
                                        elif monitored_files[file_key] != file_size:
                                            # æ–‡ä»¶å¤§å°æœ‰å˜åŒ–ï¼Œè¯´æ˜æ­£åœ¨ä¸‹è½½
                                            monitored_files[file_key] = file_size

                                    except Exception as e:
                                        logger.debug(f"Error getting file size for {file_path}: {e}")

                        if srr_size > 0:
                            found_files += 1
                            total_size += srr_size

                    # å¦‚æœæœ‰æ–‡ä»¶å¤§å°å˜åŒ–ï¼Œæ›´æ–°è¿›åº¦æ¡
                    if total_size > last_total_size:
                        progress_bar.update(total_size)
                        last_total_size = total_size
                        logger.debug(f"Total download progress: {self._format_size(total_size)} ({found_files}/{len(srr_list)} files)")

                    start_time = current_time

                time.sleep(0.5)  # å‡å°‘CPUä½¿ç”¨ç‡

        except Exception as e:
            logger.debug(f"File monitoring error: {e}")
        finally:
            logger.debug("File size monitoring stopped")

    def _parse_size_to_bytes(self, size: float, unit: str) -> int:
        """å°†å¤§å°å­—ç¬¦ä¸²è½¬æ¢ä¸ºå­—èŠ‚æ•°"""
        unit = unit.upper()
        multipliers = {
            'B': 1,
            'KB': 1024,
            'MB': 1024 ** 2,
            'GB': 1024 ** 3,
            'TB': 1024 ** 4
        }
        return int(size * multipliers.get(unit, 1))

    # ---------- SRA: iseq download ----------
    def iseq_download(self, srr_list: Sequence[str], **iseq_options) -> Sequence[Path]:
        """
        Download SRA data using iseq tool.
        Returns: list of downloaded file paths.

        Args:
            srr_list: List of SRR accessions
            **iseq_options: Additional iseq options like gzip=True, aspera=True, etc.
        """
        # Check if iseq is available
        from . import tools_check as _tools_check

        # First check and auto-install axel if needed (Jupyter Lab specific)
        logger.info("Checking axel availability for iseq...")
        axel_available, axel_path = _tools_check.check_axel(auto_install=True)
        if not axel_available:
            logger.warning(f"axel is not available, which may cause issues with iseq: {axel_path}")
            # Continue anyway, as iseq might work without axel for some databases
        else:
            logger.info(f"axel is available: {axel_path}")

        # Check iseq availability
        iseq_available, iseq_path = _tools_check.check_iseq()
        if not iseq_available:
            raise RuntimeError(f"iseq not found: {iseq_path}. Please install with: conda install -c bioconda iseq -y")

        # Create output directory
        out_dir = Path(self.cfg.prefetch_root)
        out_dir.mkdir(parents=True, exist_ok=True)

        # Build iseq command
        # For multiple accessions, create a file with one accession per line
        if len(srr_list) == 1:
            # Single accession - pass directly
            cmd = [iseq_path, "-i", srr_list[0]]
        else:
            # Multiple accessions - create input file
            input_file = out_dir / "iseq_input.txt"
            input_file.write_text("\n".join(srr_list))
            cmd = [iseq_path, "-i", str(input_file)]

        # Add default options from configuration
        cmd.extend(["-o", str(out_dir)])
        cmd.extend(["-t", str(self.cfg.iseq_threads)])

        # Add configuration-based options
        if self.cfg.iseq_gzip:
            cmd.append("-g")

        # Aspera only supported for GSA/ENA databases, not SRA
        if self.cfg.iseq_aspera and self.cfg.iseq_database in ["ena", "gsa"]:
            cmd.append("-a")

        if self.cfg.iseq_parallel > 1:
            cmd.extend(["-p", str(self.cfg.iseq_parallel)])
        if self.cfg.iseq_database:
            cmd.extend(["-d", self.cfg.iseq_database])

        # Protocol parameter only for ENA database
        if self.cfg.iseq_protocol and self.cfg.iseq_database == "ena":
            cmd.extend(["-r", self.cfg.iseq_protocol])

        # Add user-specified options (override config)
        if iseq_options.get('gzip') is not None:
            if iseq_options['gzip']:
                cmd.append("-g")
            elif "-g" in cmd:
                cmd.remove("-g")

        # Handle Aspera - only for ENA/GSA databases
        if iseq_options.get('aspera') is not None:
            if iseq_options['aspera']:
                # æ£€æŸ¥å½“å‰æ•°æ®åº“è®¾ç½®
                current_db = iseq_options.get('database', self.cfg.iseq_database)
                if current_db in ["ena", "gsa"]:
                    cmd.append("-a")
                else:
                    logger.warning(f"Asperaä¸æ”¯æŒ{current_db}æ•°æ®åº“ï¼Œè·³è¿‡-aå‚æ•°")
            elif "-a" in cmd:
                cmd.remove("-a")

        if iseq_options.get('parallel'):
            cmd.extend(["-p", str(iseq_options['parallel'])])

        if iseq_options.get('database'):
            new_db = iseq_options['database']
            cmd.extend(["-d", new_db])
            # å¦‚æœåˆ‡æ¢åˆ°éENAæ•°æ®åº“ï¼Œéœ€è¦ç§»é™¤åè®®å‚æ•°
            if new_db != "ena" and "-r" in cmd:
                cmd.remove("-r")
                cmd.remove(cmd[cmd.index("-r") + 1])  # ç§»é™¤åè®®å€¼

        # åè®®å‚æ•°ä»…é€‚ç”¨äºENAæ•°æ®åº“
        if iseq_options.get('protocol'):
            current_db = iseq_options.get('database', self.cfg.iseq_database)
            if current_db == "ena":
                cmd.extend(["-r", iseq_options['protocol']])
            else:
                logger.warning(f"åè®®å‚æ•°ä»…é€‚ç”¨äºENAæ•°æ®åº“ï¼Œå½“å‰æ•°æ®åº“ä¸º{current_db}ï¼Œè·³è¿‡-rå‚æ•°")

        #if iseq_options.get('quiet', True):
        #    cmd.append("-Q")

        logger.info(f"Running iseq command: {' '.join(cmd)}")

        # Run iseq with real-time progress monitoring
        try:
            # ä½¿ç”¨merged_envç¡®ä¿å­è¿›ç¨‹æœ‰æ­£ç¡®çš„ç¯å¢ƒå˜é‡
            from . import tools_check as _tools_check

            logger.info("Starting iseq download with real-time progress monitoring...")

            # åˆ›å»ºè¿›åº¦æ¡
            progress_bar = DownloadProgressBar(desc=f"ğŸ“¥ Downloading {len(srr_list)} SRA file(s)")

            # å¯åŠ¨æ–‡ä»¶å¤§å°ç›‘æ§çº¿ç¨‹
            file_monitor_thread = None
            if len(srr_list) <= 5:  # åªå¯¹å°‘é‡æ–‡ä»¶å¯ç”¨æ–‡ä»¶ç›‘æ§
                file_monitor_thread = threading.Thread(
                    target=self._monitor_file_sizes,
                    args=(out_dir, srr_list, progress_bar)
                )
                file_monitor_thread.daemon = True
                file_monitor_thread.start()

            # å¯åŠ¨è¿›ç¨‹
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                bufsize=1,
                universal_newlines=True,
                env=_tools_check.merged_env()
            )

            # å®æ—¶ç›‘æ§è¾“å‡º
            def monitor_output(pipe, pipe_name):
                """ç›‘æ§è¾“å‡ºæµå¹¶è§£æè¿›åº¦ä¿¡æ¯"""
                try:
                    for line in iter(pipe.readline, ''):
                        line = line.strip()
                        if line:
                            logger.debug(f"iseq {pipe_name}: {line}")

                            # å°è¯•è§£æè¿›åº¦ä¿¡æ¯
                            # iseq å¯èƒ½è¾“å‡ºç±»ä¼¼: "Downloading: 45.2MB/125.8MB (36%)" æˆ– "45.2MB downloaded"
                            self._parse_progress_line(line, progress_bar)

                except Exception as e:
                    logger.debug(f"Error monitoring {pipe_name}: {e}")
                finally:
                    pipe.close()

            # å¯åŠ¨ç›‘æ§çº¿ç¨‹
            stdout_thread = threading.Thread(target=monitor_output, args=(process.stdout, "stdout"))
            stderr_thread = threading.Thread(target=monitor_output, args=(process.stderr, "stderr"))
            stdout_thread.daemon = True
            stderr_thread.daemon = True
            stdout_thread.start()
            stderr_thread.start()

            # ç­‰å¾…è¿›ç¨‹å®Œæˆ
            return_code = process.wait()

            # ç­‰å¾…ç›‘æ§çº¿ç¨‹å®Œæˆ
            stdout_thread.join(timeout=2)
            stderr_thread.join(timeout=2)

            # åœæ­¢æ–‡ä»¶ç›‘æ§
            if file_monitor_thread:
                file_monitor_thread.join(timeout=1)

            # å®Œæˆè¿›åº¦æ¡
            progress_bar.finish()

            if return_code != 0:
                # æ”¶é›†é”™è¯¯ä¿¡æ¯
                error_output = []
                if process.stderr:
                    try:
                        error_output.append(process.stderr.read())
                    except:
                        pass

                error_msg = " ".join(error_output) if error_output else f"Process exited with code {return_code}"
                logger.error("iseq download failed: %s", error_msg)
                raise RuntimeError(f"iseq download failed: {error_msg}")

        except Exception as e:
            if 'progress_bar' in locals():
                progress_bar.stop()
            logger.error("iseq download failed: %s", str(e))
            raise RuntimeError(f"iseq download failed: {str(e)}") from e

        # Find downloaded files with better detection
        downloaded_files = []
        missing_accessions = []

        for srr in srr_list:
            # Look for files with multiple naming patterns
            patterns = [
                f"{srr}*.fastq",      # Standard FASTQ
                f"{srr}*.fastq.gz",   # Gzipped FASTQ
                f"{srr}*.fq",         # Short extension
                f"{srr}*.fq.gz",      # Short extension gzipped
                f"{srr}*.sra",        # SRA format (fallback)
                f"{srr}*.sralite"     # SRA lite format
            ]

            srr_files = []
            for pattern in patterns:
                files = list(out_dir.glob(pattern))
                if files:
                    srr_files.extend(files)
                    logger.info(f"Found {len(files)} files for {srr} with pattern {pattern}")

            if srr_files:
                downloaded_files.extend(srr_files)
                logger.info(f"Found {len(srr_files)} total files for {srr}")
            else:
                missing_accessions.append(srr)
                logger.warning(f"No files found for {srr}")

        # Only report success if we actually found files
        if downloaded_files:
            logger.info(f"iseq download completed successfully - found {len(downloaded_files)} files")
        else:
            logger.error(f"iseq download failed - no files found for any accessions: {srr_list}")
            raise RuntimeError(f"iseq download failed - no files found for accessions: {missing_accessions}")

        return [Path(f) for f in downloaded_files]

    # ---------- SRA: prefetch ----------
    def prefetch(self, srr_list: Sequence[str]) -> Sequence[Path]:
        """
        Prefetch SRAs to .sra files.
        Returns: list of .sra paths.

        æ–°å¢ï¼šæ”¯æŒé•œåƒè‡ªåŠ¨åˆ‡æ¢ï¼Œé€šè¿‡é…ç½®ä¸­çš„ mirror_config å‚æ•°
        """
        # Delegate to your prefetch implementation. We assume it exposes a function like:
        # _sra_prefetch.prefetch_batch(srr_list, out_root=..., threads=..., mirror_config=...)
        if not hasattr(_sra_prefetch, "prefetch_batch"):
            raise RuntimeError("sra_prefetch.prefetch_batch(...) not found. Please expose it.")
        return _sra_prefetch.prefetch_batch(
            srr_list=srr_list,
            out_root=str(self.cfg.prefetch_root),
            threads=self.cfg.threads,
            prefetch_config=self.cfg.prefetch_config  # ä½¿ç”¨åŸºæœ¬é¢„å–é…ç½®
        )

    # ---------- SRA: fasterq-dump ----------
    def fasterq(self, srr_list: Sequence[str]) -> Sequence[Tuple[str, Path, Path]]:
        """
        Convert SRA to paired FASTQ(.gz).
        Returns: list of tuples (srr, fq1_path, fq2_path).
        """
        if not hasattr(_sra_fasterq, "fasterq_batch"):
            raise RuntimeError("sra_fasterq.fasterq_batch(...) not found. Please expose it.")
        return _sra_fasterq.fasterq_batch(
            srr_list=srr_list,
            out_root=str(self.cfg.fasterq_root),
            threads=self.cfg.threads,
            mem = self.cfg.memory,
            gzip_output=self.cfg.gzip_fastq,
            tmp_root=str(self.cfg.fasterq_root / "tmp")  # Use proper tmp directory within fasterq_root
        )

    # ---------- QC: fastp ----------
    def fastp(self, fq_pairs: Sequence[Tuple[str, Path, Path]]) -> Sequence[Tuple[str, Path, Path]]:
        """
        Run fastp on paired FASTQ files.
        Returns: list of tuples (srr, fq1_qc, fq2_qc).
        """
        if not self.cfg.fastp_enabled:
            # pass-through without QC
            return [(srr, fq1, fq2) for srr, fq1, fq2 in fq_pairs]

        if not hasattr(_qc_fastp, "fastp_batch"):
            raise RuntimeError("qc_fastp.fastp_batch(...) not found. Please expose it.")
        return _qc_fastp.fastp_batch(
            pairs=fq_pairs,
            out_root=str(self.cfg.qc_root),
            threads=self.cfg.threads
        )

    # ---------- Alignment (placeholder) ----------
    def star_align(
        self,
        clean_fastqs: Sequence[Tuple[str, Path, Path]],
        *,
        gencode_release: str = "v44",
        sjdb_overhang: Optional[int] = 149,
        index_root: Optional[Path] = None,
        accession_for_species: Optional[str] = None,   # æ‰€æœ‰æ ·æœ¬åŒä¸€ GSE æ—¶å¯ç»Ÿä¸€ä¼ ï¼›å¦åˆ™ä¿æŒ None
        max_workers: Optional[int] = None,             # åŒæ—¶è·‘å¤šå°‘ä¸ªæ ·æœ¬ï¼›None=ä¸²è¡Œï¼Œæ—¥å¿—æ›´æ¸…æ™°
    ) -> list[Tuple[str, str, Optional[str]]]:
        """
        æ‰¹é‡è·‘ STARï¼ˆè°ƒç”¨ batch ç‰ˆ star_stepï¼‰ï¼Œè¿”å›ï¼š
          [(srr, bam_path, index_dir|None), ...]
        - å¹‚ç­‰ï¼šè‹¥ <SRR>/Aligned.sortedByCoord.out.bam å·²å­˜åœ¨ä¸”>1MBï¼Œåˆ™ [SKIP]
        - index_dir è‹¥åœ¨ star_tools è¿”å›ä¸­å¯è§£æåˆ™ç»™å‡ºï¼Œå¦åˆ™ä¸º Noneï¼ˆä¸ä½ åç»­ GTF æ¨æ–­é€»è¾‘ä¸€è‡´ï¼‰
        """
        if not hasattr(_star_step, "make_star_step"):
            raise RuntimeError("star_step.make_star_step(...) not found")

        # æ„é€ ä¸€æ­¥â€œå¯æ‰¹é‡â€çš„ stepï¼ˆä¸åŸæœ‰å·¥å‚æ¥å£å®Œå…¨ä¸€è‡´ï¼‰
        step = _star_step.make_star_step(
            index_root=str(self.cfg.star_index_root),
            out_root=str(self.cfg.star_align_root),
            threads=int(self.cfg.threads),
            gencode_release=gencode_release,
            sjdb_overhang=sjdb_overhang,
            accession_for_species=accession_for_species,
            max_workers=max_workers,   # None=ä¸²è¡Œï¼›ä¹Ÿå¯å¤–éƒ¨ä¼  2/4 å¹¶å‘
        )

        # è§„èŒƒè¾“å…¥ä¸º [(srr, str(fq1), str(fq2)), ...]
        # fastq_qc è¿”å›çš„æ˜¯5å…ƒç»„ï¼š(srr, fq1, fq2, json, html)ï¼Œæˆ‘ä»¬åªéœ€è¦å‰ä¸‰ä¸ª
        pairs: List[Tuple[str, str, str]] = [
            (srr, str(Path(fq1)), str(Path(fq2))) for srr, fq1, fq2, *_ in clean_fastqs
        ]

        # ç›´æ¥è°ƒç”¨æ‰¹é‡ commandï¼Œå¾—åˆ° [(srr, bam, index_dir|None), ...]
        products = step["command"](pairs, logger=None)
        # ä¸éšå pipeline çš„â€œä¸‰å…ƒç»„è§„èŒƒåŒ–â€å®Œå…¨å…¼å®¹
        return products

    # ---------- Counting via featureCounts ----------
    def featurecounts(
        self,
        bam_triples: Sequence[Tuple[str, str | Path, Optional[str]]],   # [(srr, bam, index_dir|None)]
        *,
        gtf: Optional[str | Path] = None,         # æ˜¾å¼ GTFï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        simple: Optional[bool] = None,            # Noneâ†’cfg.featurecounts_simple
        by: Optional[str] = None,                 # Noneâ†’cfg.featurecounts_by
        threads: Optional[int] = None,            # Noneâ†’cfg.threads
        max_workers: Optional[int] = None,        # é¢„ç•™ï¼ˆcount_tools å¯å¹¶è¡Œæ—¶é€ä¼ ï¼‰
    ) -> Dict[str, object]:
        """
        æ‰¹é‡è°ƒç”¨ featureCountsã€‚è¿”å›ï¼š
          { "tables": [(srr, table_path), ...], "matrix": <path|None>, "failed": [] }
        å¹‚ç­‰ï¼š<counts_root>/<SRR>/<SRR>.counts.txt å­˜åœ¨ä¸”>0åˆ™è·³è¿‡è®¡ç®—ã€‚
        """
        if not hasattr(_count_step, "make_featurecounts_step"):
            raise RuntimeError("count_step.make_featurecounts_step(...) not found")

        out_root = Path(self.cfg.counts_root)
        out_root.mkdir(parents=True, exist_ok=True)

        # ---------- å†…ç½® GTF è‡ªåŠ¨æ¨æ–­ ----------
        def _infer_gtf_from_bams(triples: Sequence[Tuple[str, str | Path, Optional[str]]]) -> Optional[str]:
            # 1) ä¼˜å…ˆï¼šä»æ¯ä¸ªæ ·æœ¬æºå¸¦çš„ index_dir æ¨æ–­
            for _srr, _bam, idx_dir in triples:
                if not idx_dir:
                    continue
                idx = Path(idx_dir)
                # (a) æœ¬ç›®å½• / çˆ¶ç›®å½•æœ *.gtf
                for base in {idx, idx.parent}:
                    for p in base.glob("*.gtf"):
                        return str(p.resolve())
                # (b) _cache ä¸‹æœ *.gtf
                for base in {idx.parent, idx.parent.parent}:
                    cache = base / "_cache"
                    if cache.exists():
                        hits = list(cache.rglob("*.gtf"))
                        if hits:
                            return str(hits[0].resolve())
                # (c) å†å‘ä¸Šä¸€çº§è¡¥å……ä¸€è½®
                for p in idx.parent.parent.glob("*.gtf"):
                    return str(p.resolve())

            # 2) å…¶æ¬¡ï¼šä»é…ç½®çš„ star_index_root ä¸‹å…œåº•æœç´¢
            idx_root = Path(getattr(self.cfg, "star_index_root", "index"))
            if idx_root.exists():
                hits = list(idx_root.rglob("*.gtf"))
                if hits:
                    return str(hits[0].resolve())

            # 3) æœ€åï¼šç¯å¢ƒå˜é‡ FC_GTF_HINT
            env_hint = os.environ.get("FC_GTF_HINT")
            if env_hint and Path(env_hint).exists():
                return str(Path(env_hint).resolve())

            return None

        # è‹¥æœªæ˜¾å¼ç»™ gtfï¼Œåˆ™è‡ªåŠ¨æ¨æ–­
        if gtf is None:
            inferred = _infer_gtf_from_bams(bam_triples)
            if inferred:
                print(f"[INFO] featureCounts: inferred GTF -> {inferred}")
                gtf = inferred
            else:
                raise RuntimeError(
                    "[featureCounts] æ— æ³•è‡ªåŠ¨æ‰¾åˆ° GTFï¼Œè¯·æ˜¾å¼ä¼ å…¥ gtf= æˆ–è®¾ç½®ç¯å¢ƒå˜é‡ FC_GTF_HINTã€‚"
                )

        # ---------- æ„å»º step å·¥å‚å¹¶å¹‚ç­‰æ£€æŸ¥ ----------
        step = _count_step.make_featurecounts_step(
            out_root=str(out_root),
            simple=(self.cfg.simple_counts if simple is None else bool(simple)),
            gtf=None,  # è¿è¡Œæ—¶ gtf é€šè¿‡ command(...) ä¼ å…¥ï¼Œä¼˜å…ˆçº§æœ€é«˜
            by=(by or self.cfg.by),
            threads=int(threads or self.cfg.threads),
            gtf_path=str(gtf),  # ä½œä¸ºå·¥å‚çš„åå¤‡ï¼ˆå†…éƒ¨ä¼˜å…ˆç”¨ command çš„ gtfï¼‰
        )

        def _table_path_for(srr: str) -> Path:
            # è‹¥ä½ çš„ count_tools äº§ç‰©å®é™…æ˜¯ .csvï¼Œè¿™é‡Œæ”¹æˆ .csv å¹¶åŒæ­¥æ”¹ outputs æ¨¡æ¿
            return out_root / srr / f"{srr}.counts.txt"

        # å¹‚ç­‰ï¼šå…¨éƒ¨å·²æœ‰åˆ™è·³è¿‡
        outs_by_srr: List[Tuple[str, Path]] = [(str(srr), _table_path_for(str(srr))) for srr, _bam, _ in bam_triples]
        if all(step["validation"]([str(p)]) for _, p in outs_by_srr):
            print("[SKIP] featureCounts for all")
            tables = [(srr, str(p)) for srr, p in outs_by_srr]
            return {"tables": tables, "matrix": None, "failed": []}

        # ç»„è£… (srr, bam) åˆ—è¡¨å¹¶è¿è¡Œ
        bam_pairs = [(str(srr), str(bam)) for (srr, bam, _idx) in bam_triples]
        ret = step["command"](
            bam_pairs,
            logger=None,
            gtf=str(gtf),  # æ˜¾å¼ä¼ å…¥ï¼Œä¼˜å…ˆçº§æœ€é«˜
        )

        tables = [(srr, str(_table_path_for(str(srr)))) for srr, _ in bam_pairs]
        matrix_path = ret.get("matrix") if isinstance(ret, dict) else None
        return {"tables": tables, "matrix": matrix_path, "failed": []}

    # ---------- iSeq: Company FASTQ Data Processing ----------
    def process_company_fastq(
        self,
        fastq_input: str | Path | List[str | Path],
        *,
        sample_prefix: Optional[str] = None,
        output_subdir: str = "iseq"
    ) -> Dict[str, Any]:
        """
        å¤„ç†å…¬å¸æä¾›çš„FASTQæ•°æ®

        Args:
            fastq_input: FASTQæ–‡ä»¶è·¯å¾„æˆ–è·¯å¾„åˆ—è¡¨ï¼Œå¯ä»¥æ˜¯æ–‡ä»¶æˆ–ç›®å½•
            sample_prefix: æ ·æœ¬IDå‰ç¼€ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„å€¼
            output_subdir: è¾“å‡ºå­ç›®å½•

        Returns:
            å¤„ç†ç»“æœå­—å…¸ï¼ŒåŒ…å«æ ·æœ¬ä¿¡æ¯å’Œæ–‡ä»¶è·¯å¾„
        """
        if not self.cfg.iseq_enabled:
            raise ValueError("iSeq processing is not enabled. Set iseq_enabled=True in config.")

        # è®¾ç½®è¾“å‡ºç›®å½•
        iseq_root = self.cfg.work_root / output_subdir
        iseq_root.mkdir(parents=True, exist_ok=True)

        # å¤„ç†è¾“å…¥è·¯å¾„
        if isinstance(fastq_input, (str, Path)):
            input_paths = [Path(fastq_input)]
        else:
            input_paths = [Path(p) for p in fastq_input]

        # å¤„ç†å…¬å¸æ•°æ®
        result = self.iseq_handler.process_company_data(
            input_paths[0] if len(input_paths) == 1 else input_paths,
            output_dir=iseq_root,
            sample_prefix=sample_prefix or self.cfg.iseq_sample_prefix
        )

        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼ï¼Œä¸SRAæµç¨‹å…¼å®¹
        sample_pairs = []
        for original_id, r1_path, r2_path in result['sample_pairs']:
            # ä½¿ç”¨æ ‡å‡†åŒ–çš„æ ·æœ¬ID
            standardized_id = result['sample_id_mapping'].get(original_id, {}).get('standardized_id', original_id)
            sample_pairs.append((standardized_id, r1_path, r2_path))

        return {
            'type': 'iseq',
            'sample_pairs': sample_pairs,
            'metadata': result['metadata_df'],
            'mapping': result['sample_id_mapping'],
            'output_dir': iseq_root
        }

    def run_from_fastq(
        self,
        fastq_pairs: Sequence[Tuple[str, Path, Optional[Path]]],
        *,
        with_align: bool = True,
        align_index: Optional[Path] = None
    ) -> Dict[str, Any]:
        """
        ä»FASTQæ–‡ä»¶å¼€å§‹è¿è¡Œå®Œæ•´æµç¨‹

        Args:
            fastq_pairs: [(sample_id, fq1_path, fq2_path), ...]
            with_align: æ˜¯å¦è¿›è¡Œæ¯”å¯¹æ­¥éª¤
            align_index: æ¯”å¯¹ç´¢å¼•è·¯å¾„

        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        # ç›´æ¥è¿›è¡ŒQCæ­¥éª¤
        fastqs_qc = self.fastp(fastq_pairs)

        result: dict[str, Any] = {
            "type": "fastq_direct",
            "fastq_input": fastq_pairs,
            "fastq_qc": fastqs_qc,
        }

        if with_align:
            bams = self.star_align(fastqs_qc, index_root=align_index)
            counts = self.featurecounts(bams, gtf=self.cfg.gtf)
            result["bam"] = bams
            result["counts"] = counts
        else:
            # å¦‚æœä¸æ¯”å¯¹ï¼Œå¯ä»¥ç›´æ¥è¿›è¡Œå®šé‡ï¼ˆéœ€è¦kallistoç­‰å·¥å…·ï¼‰
            logger.warning("Direct quantification from FASTQ without alignment is not implemented yet.")

        return result

    # ---------- Unified Pipeline Entry ----------
    def run_pipeline(
        self,
        input_data: Union[str, Path, List[str], List[Path], Sequence[str]],
        *,
        input_type: Literal["sra", "fastq", "company"] = "auto",
        with_align: bool = True,
        align_index: Optional[Path] = None,
        sample_prefix: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ç»Ÿä¸€çš„ç®¡é“å…¥å£ï¼Œæ”¯æŒå¤šç§è¾“å…¥ç±»å‹

        Args:
            input_data: è¾“å…¥æ•°æ®ï¼Œå¯ä»¥æ˜¯ï¼š
                - SRA: SRRç¼–å·åˆ—è¡¨æˆ–GEO accession
                - FASTQ: FASTQæ–‡ä»¶è·¯å¾„æˆ–è·¯å¾„åˆ—è¡¨
                - Company: å…¬å¸æ•°æ®ç›®å½•æˆ–æ–‡ä»¶åˆ—è¡¨
            input_type: è¾“å…¥ç±»å‹ï¼Œå¯é€‰ "sra", "fastq", "company", "auto"
            with_align: æ˜¯å¦è¿›è¡Œæ¯”å¯¹æ­¥éª¤
            align_index: æ¯”å¯¹ç´¢å¼•è·¯å¾„
            sample_prefix: æ ·æœ¬å‰ç¼€ï¼ˆä»…å¯¹å…¬å¸æ•°æ®æœ‰æ•ˆï¼‰

        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        # è‡ªåŠ¨æ£€æµ‹è¾“å…¥ç±»å‹
        if input_type == "auto":
            input_type = self._detect_input_type(input_data)

        logger.info(f"Detected input type: {input_type}")

        if input_type == "sra":
            # SRAæ•°æ® - ä½¿ç”¨åŸæœ‰æµç¨‹
            # æ”¯æŒtxtæ–‡ä»¶è¾“å…¥ï¼Œç›´æ¥è¯»å–å…¶ä¸­çš„accessionåˆ—è¡¨
            if isinstance(input_data, str) and Path(input_data).exists() and Path(input_data).suffix == '.txt':
                # å¦‚æœæ˜¯txtæ–‡ä»¶ï¼Œè¯»å–å…¶ä¸­çš„accessionåˆ—è¡¨
                srr_list = self._read_sra_accessions_from_file(Path(input_data))
            elif isinstance(input_data, str):
                srr_list = [input_data]  # å•ä¸ªå­—ç¬¦ä¸²åŒ…è£…æˆåˆ—è¡¨
            elif isinstance(input_data, (list, tuple)):
                srr_list = list(input_data)  # è½¬æ¢åˆ—è¡¨æ ¼å¼
            else:
                srr_list = list(input_data)  # å…¶ä»–åºåˆ—ç±»å‹

            return self.run(srr_list=srr_list, with_align=with_align, align_index=align_index)

        elif input_type == "company":
            # å…¬å¸æ•°æ® - å…ˆå¤„ç†æ•°æ®ï¼Œå†è¿è¡Œæµç¨‹
            iseq_result = self.process_company_fastq(
                input_data,
                sample_prefix=sample_prefix
            )
            return self.run_from_fastq(
                iseq_result['sample_pairs'],
                with_align=with_align,
                align_index=align_index
            )

        elif input_type == "fastq":
            # ç›´æ¥FASTQæ–‡ä»¶ - éœ€è¦ç”¨æˆ·æŒ‡å®šæ ·æœ¬ID
            fastq_pairs = self._parse_fastq_input(input_data)
            return self.run_from_fastq(
                fastq_pairs,
                with_align=with_align,
                align_index=align_index
            )

        else:
            raise ValueError(f"Unsupported input type: {input_type}")

    def _detect_input_type(self, input_data: Any) -> str:
        """è‡ªåŠ¨æ£€æµ‹è¾“å…¥ç±»å‹"""
        if isinstance(input_data, str):
            # å•ä¸ªå­—ç¬¦ä¸²
            if input_data.startswith(('SRR', 'ERR', 'DRR')):
                return "sra"
            elif Path(input_data).exists():
                # å­˜åœ¨çš„è·¯å¾„
                path = Path(input_data)
                if path.is_file() and any(str(path).endswith(ext) for ext in ['.fq', '.fastq', '.fq.gz', '.fastq.gz']):
                    return "fastq"
                elif path.suffix in ['.txt', '.csv']:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯åŒ…å«SRA accessionçš„æ–‡æœ¬æ–‡ä»¶
                    if self._is_sra_accession_file(path):
                        return "sra"
                    else:
                        return "company"
                elif path.is_dir():
                    return "company"
                else:
                    return "fastq"
            else:
                # å¯èƒ½æ˜¯GEO accession
                return "sra"

        elif isinstance(input_data, Path):
            # Pathå¯¹è±¡
            if input_data.exists():
                if input_data.is_file() and any(str(input_data).endswith(ext) for ext in ['.fq', '.fastq', '.fq.gz', '.fastq.gz']):
                    return "fastq"
                else:
                    return "company"
            else:
                return "sra"

        elif isinstance(input_data, (list, tuple)):
            # åˆ—è¡¨æˆ–å…ƒç»„
            if len(input_data) == 0:
                raise ValueError("Empty input data")

            first_item = input_data[0]
            if isinstance(first_item, str) and first_item.startswith(('SRR', 'ERR', 'DRR')):
                return "sra"
            elif any(str(item).endswith(('.fq', '.fastq', '.fq.gz', '.fastq.gz')) for item in input_data):
                return "fastq"
            else:
                return "company"

        else:
            raise ValueError(f"Cannot detect input type for: {type(input_data)}")

    def _is_sra_accession_file(self, file_path: Path) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åŒ…å«SRA accessionåˆ—è¡¨"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()

            # æ£€æŸ¥å‰å‡ è¡Œæ˜¯å¦åŒ…å«SRA/ERR/DRR accession
            sra_pattern = re.compile(r'^(SRR|ERR|DRR)\d+$')
            valid_lines = 0

            for line in lines[:20]:  # æ£€æŸ¥å‰20è¡Œ
                line = line.strip()
                if line and not line.startswith('#'):  # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Šè¡Œ
                    if sra_pattern.match(line):
                        valid_lines += 1
                    else:
                        return False  # å¦‚æœæœ‰ä¸€è¡Œä¸æ˜¯SRAæ ¼å¼ï¼Œè¿”å›False

            # è‡³å°‘æœ‰ä¸€è¡Œæœ‰æ•ˆçš„SRA accession
            return valid_lines > 0

        except Exception:
            return False

    def _read_sra_accessions_from_file(self, file_path: Path) -> List[str]:
        """ä»txtæ–‡ä»¶ä¸­è¯»å–SRA accessionåˆ—è¡¨"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()

            srr_list = []
            sra_pattern = re.compile(r'^(SRR|ERR|DRR)\d+$')

            for line in lines:
                line = line.strip()
                # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Šè¡Œ
                if line and not line.startswith('#'):
                    if sra_pattern.match(line):
                        srr_list.append(line)
                    else:
                        logger.warning(f"è·³è¿‡æ— æ•ˆçš„è¡Œ: {line}")

            if not srr_list:
                raise ValueError(f"æ–‡ä»¶ {file_path} ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„SRA accession")

            logger.info(f"ä»æ–‡ä»¶ {file_path} ä¸­è¯»å–åˆ° {len(srr_list)} ä¸ªSRA accession")
            return srr_list

        except Exception as e:
            raise RuntimeError(f"è¯»å–SRA accessionæ–‡ä»¶å¤±è´¥ {file_path}: {e}")

    def _parse_fastq_input(self, input_data: Union[str, Path, List[str], List[Path]]) -> List[Tuple[str, Path, Optional[Path]]]:
        """è§£æFASTQè¾“å…¥æ•°æ®"""
        if isinstance(input_data, (str, Path)):
            # å•ä¸ªæ–‡ä»¶
            file_path = Path(input_data)
            if not file_path.exists():
                raise FileNotFoundError(f"File not found: {file_path}")

            # è‡ªåŠ¨æå–æ ·æœ¬ID
            sample_id = self.iseq_handler.extract_sample_id(file_path)

            # æ£€æŸ¥æ˜¯å¦ä¸ºåŒç«¯æµ‹åºæ–‡ä»¶
            if self._is_paired_end_file(file_path):
                # éœ€è¦æ‰¾åˆ°å¯¹åº”çš„R2æ–‡ä»¶
                r2_path = self._find_paired_file(file_path, "R2")
                return [(sample_id, file_path, r2_path)]
            else:
                return [(sample_id, file_path, None)]

        elif isinstance(input_data, (list, tuple)):
            # å¤šä¸ªæ–‡ä»¶
            fastq_files = [Path(f) for f in input_data]

            # ä½¿ç”¨iSeqå¤„ç†å™¨è¿›è¡Œé…å¯¹
            return self.iseq_handler.group_paired_end(fastq_files)

        else:
            raise ValueError(f"Unsupported input format: {type(input_data)}")

    def _is_paired_end_file(self, file_path: Path) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºåŒç«¯æµ‹åºçš„R1æ–‡ä»¶"""
        filename = file_path.name
        return bool(re.search(r'[._][Rr]1[._]', filename))

    def _find_paired_file(self, r1_path: Path, direction: str) -> Optional[Path]:
        """æŸ¥æ‰¾é…å¯¹çš„R2æ–‡ä»¶"""
        r1_name = r1_path.name

        # æ„å»ºR2æ–‡ä»¶å
        if direction == "R2":
            r2_name = re.sub(r'([._])[Rr]1([._])', r'\1R2\2', r1_name)
            if r2_name == r1_name:  # å¦‚æœæ²¡æœ‰æ›¿æ¢æˆåŠŸ
                r2_name = r1_name.replace("_R1", "_R2").replace(".R1", ".R2")
        else:
            return None

        r2_path = r1_path.parent / r2_name
        return r2_path if r2_path.exists() else None

    def run(self, srr_list: Sequence[str], *, with_align: bool = False, align_index: Optional[Path] = None) -> Dict[str, Any]:
        """
        Convenience runner: download -> fasterq -> (fastp) -> [align] -> featureCounts

        Supports both prefetch and iseq download methods.

        Returns a dict of paths keyed by step.
        """
        # Choose download method based on configuration
        if self.cfg.download_method == "iseq":
            logger.info(f"Using iseq for download with {len(srr_list)} accessions")
            downloaded_files = self.iseq_download(srr_list)

            # Check if we got FASTQ files directly from iseq
            if downloaded_files:
                # Check if any files are FASTQ format
                fastq_files = [f for f in downloaded_files if str(f).endswith(('.fastq', '.fastq.gz', '.fq', '.fq.gz'))]
                sra_files = [f for f in downloaded_files if str(f).endswith(('.sra', '.sralite'))]

                if fastq_files:
                    # iseq provided FASTQ files, skip fasterq-dump step
                    logger.info(f"iseq provided {len(fastq_files)} FASTQ files, skipping fasterq-dump step")

                    # Create fasterq results from iseq output
                    fastqs = []
                    for srr in srr_list:
                        # Find files for this specific SRR
                        srr_files = [f for f in fastq_files if srr in str(f)]

                        if len(srr_files) >= 2:
                            # Try to find R1/R2 paired files
                            r1_candidates = [f for f in srr_files if any(marker in str(f) for marker in ['_1', '.1.', '_R1', '.R1.'])]
                            r2_candidates = [f for f in srr_files if any(marker in str(f) for marker in ['_2', '.2.', '_R2', '.R2.'])]

                            if r1_candidates and r2_candidates:
                                # Use the first matching R1/R2 files
                                fastqs.append((srr, Path(r1_candidates[0]), Path(r2_candidates[0])))
                            else:
                                # Fallback: use first two files
                                fastqs.append((srr, Path(srr_files[0]), Path(srr_files[1])))
                        elif srr_files:
                            # Single-end or single file
                            fastqs.append((srr, Path(srr_files[0]), None))
                        else:
                            logger.warning(f"No FASTQ files found for {srr} in iseq output")

                    result: dict[str, Any] = {
                        "prefetch": downloaded_files,  # Store iseq downloads as prefetch equivalent
                        "fastq": fastqs,
                    }
                elif sra_files:
                    # iseq downloaded SRA files, need fasterq-dump conversion
                    logger.info(f"iseq provided {len(sra_files)} SRA files, will run fasterq-dump conversion")
                    sras = downloaded_files
                    fastqs = self.fasterq(srr_list)
                    result: dict[str, Any] = {
                        "prefetch": sras,
                        "fastq": fastqs,
                    }
                else:
                    # Unknown file types, try fasterq-dump anyway
                    logger.warning(f"iseq provided files of unknown type, will attempt fasterq-dump conversion")
                    sras = downloaded_files
                    fastqs = self.fasterq(srr_list)
                    result: dict[str, Any] = {
                        "prefetch": sras,
                        "fastq": fastqs,
                    }
            else:
                # No files found, this should have been caught by iseq_download
                logger.error("iseq download returned no files")
                raise RuntimeError("iseq download failed - no files were downloaded")
        else:
            # Default prefetch method
            logger.info(f"Using prefetch for download with {len(srr_list)} accessions")
            sras = self.prefetch(srr_list)
            fastqs = self.fasterq(srr_list)
            result: dict[str, Any] = {
                "prefetch": sras,
                "fastq": fastqs,
            }

        # Continue with common processing
        fastqs_qc = self.fastp(fastqs)
        result["fastq_qc"] = fastqs_qc

        if with_align:
            # ä½¿ç”¨ç°æœ‰çš„STARæ¯”å¯¹åŠŸèƒ½
            bam_triples = self.star_align(fastqs_qc)
            # æå–BAMè·¯å¾„ï¼ˆbam_triplesæ ¼å¼: [(srr, bam_path, index_dir), ...]ï¼‰
            bams = [(srr, Path(bam_path)) for srr, bam_path, _ in bam_triples]
        else:
            # è·³è¿‡æ¯”å¯¹æ­¥éª¤ï¼Œç›´æ¥è¿”å›ç©ºç»“æœ
            logger.info("è·³è¿‡æ¯”å¯¹æ­¥éª¤ (with_align=False)")
            bam_triples = []
            bams = []

        counts = self.featurecounts(bam_triples, gtf=self.cfg.gtf)
        result["bam"] = bams
        result["counts"] = counts
        return result
